---
title: "FAUNMAP audit"
author: "Nick Hoffman"
date: '`r Sys.Date()`'
output: 
  html_document:
    df_print: paged
    css: db_by_db.css
    toc: true
    number_sections: true
    toc_depth: 1
    toc_float: true
    theme: journal
---

```{r, results='asis', echo = F}
toc_depth <- rmarkdown::metadata$output$html_document$toc_depth
sel <- paste0("h",(toc_depth+1):10, collapse = " > span, ")
cat(paste0("<style>",
           sel, 
           " > .header-section-number { display: none; } </style>"))
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(dplyr)
library(DT)
library(geojsonsf)
library(osmdata)
library(rosm)
library(rmapshaper)
library(tmap)
library(sf)
library(leaflet)
library(stringr)
library(wkb)
library(tidyverse)
library(neotoma2)
library(lubridate)
```

```{r neotables, echo=FALSE,message=FALSE,warning=FALSE}



tables = c("datasets","publications", "datasetpublications","collectionunits","datasetdatabases","constituentdatabases","sites","depenvttypes")



for (i in seq(length(tables))) {

  table = tables[[i]]
dslinks = content(GET(paste0("https://api.neotomadb.org/v2.0/data/dbtables?table=",table,"&limit=75000&offset=0")))$data



if (table == "publications") {
  dsl_df = matrix(nrow=length(dslinks),ncol=(length(dslinks[[1]]) - 2))
  for (j in seq(1,length(dslinks))) {
  for (k in seq((length(dslinks[[1]])) - 2)) {
    if (!is.null(dslinks[[j]][[k]])) {
      dsl_df[j,k] = dslinks[[j]][[k]]
    }
  }
}
}
else {
  dsl_df = matrix(nrow=length(dslinks),ncol=length(dslinks[[1]]))
for (j in seq(1,length(dslinks))) {
  for (k in seq(length(dslinks[[1]]))) {
    if (!is.null(dslinks[[j]][[k]])) {
      dsl_df[j,k] = dslinks[[j]][[k]]
    }
  }
}
}

dsl_df = as.data.frame(dsl_df)

if (table =="publications") {
names(dsl_df) = names(dslinks[[1]])[1:(length(dslinks[[1]])-2)]  }

else {
  names(dsl_df) = names(dslinks[[1]])  
}

assign(paste0(table,"_df"), dsl_df)
}

faun_sites = sites_df %>% dplyr::left_join(collectionunits_df, by=join_by(siteid)) %>% left_join(depenvttypes_df, by=join_by(depenvtid)) %>% left_join(datasets_df,by=join_by(collectionunitid)) %>% left_join(datasetdatabases_df,by=join_by(datasetid)) %>% left_join(constituentdatabases_df,by=join_by(databaseid)) %>% dplyr::filter(databaseid=='10') 


num_sites = length(faun_sites[[1]])

month = month(as.Date(Sys.Date()))
year = year(as.Date(Sys.Date()))


nums = seq(1,12)
months= c("January","February","March","April","May","June","July","August","September","October","November","December")
month_map = data.frame(nums=nums,months=months)

month = month_map %>% dplyr::filter(nums==month) %>% pull(months)
```

# Introduction

Theorists of Indigenous data sovereignty exhort stewards of data in which Indigenous peoples have rights (i.e., Indigenous data) to honor these rights by aligning with the CARE principles of Indigenous data governance (Kukutai & Taylor, 2016; Carroll, 2020). The Neotoma Paleoecology Database curates data derived from specimens which settlers have excavated from native heritage sites, sometimes unjustly. Also, the subsequent curation of these data has sometimes fallen short of best practices. Neotoma is seeking to better steward these Indigenous data and to align with the CARE principles through its participation in the Ethical Open Science for Past Global Change Data Research Coordination Network.

One of Neotoma's constituent databases is FAUNMAP: an effort begun in 1990 by vertebrate paleontologists Ernest Lundelius and Russell Graham to compile data from across the United States on mammal faunas of the Late Quaternary period for the purpose of generating paleobiological and paleoclimatic insights. By 1995, FAUNMAP consisted of over 2900 sites. In the 2000s, FAUNMAP merged with the Global Pollen Databases and others to form Neotoma. It is a living database which as of `r month` `r year` consists of `r num_sites` sites.

In order to achieve alignment with CARE, it is necessary to have a more complete accounting of 1) what Indigenous data FAUNMAP currently stewards and 2) how FAUNMAP’s stewardship of those data conforms or not to best practice. We present here an account of our attempt to perform such a data audit, as well as our preliminary findings.

We searched programmatically through FAUNMAP’s metadata for any sensitive Indigenous data. Such sensitive Indigenous data include 

* those from human (i.e., archaeological) sites
* culturally sensitive sites
* those for which purposeful imprecision has been introduced to protect sensitive sites
* those that concern Indigenous ancestors (e.g., human skeletal elements and human-derived radiocarbon dates).


Here we document these sensitive FAUNMAP records. Although limitations in FAUNMAP’s metadata led us to systematically undercount or overcount records in different circumstances, we are hopeful that this preliminary inventory will spur us to better steward these data.

<i>Note: the following audit contains potentially sensitive information about Indigenous ancestors. Our intent is to expose this information in order to work toward better management in the future.</i>



```{r smiths, echo=FALSE, message=FALSE,warning=FALSE}

# FAUNMAP's sites

#The sites table is a central element of Neotoma's data structure. We searched FAUNMAP sites in Neotoma for any which 
  
# are associated with a Smithsonian trinomial,
# are fuzzed according to the original FAUNMAP data,
# have discrepancies in their coordinates, or
# may be a sacred site.
states = read_sf("tl_2024_us_state.shp")
states = ms_simplify(states)

# https://www.digtech-llc.com/blog/2012/3/27/94-shovelbums-guide-part-15-the-smithsonian-trinomial-system.html

state_names = c("Alabama", "Alaska", "American Samoa", "Arizona", "Arkansas", "California", "Colorado","Connecticut","Delaware", "District of Columbia", "Federated States of Micronesia", "Florida", "Georgia","Guam","Hawaii","Idaho","Illinois","Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Marshall Islands", "Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Northern Mariana Islands","Ohio","Oklahoma","Oregon","Palau","Pennsylvania","Puerto Rico","Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virgin Islands","Virginia","Washington","West Virginia","Wisconsin","Wyoming")

tris = c(1,49,NA,2,3,4,5,6,7,NA,NA,8,9,NA,50,10,11,12,13,14,15,16,17,NA,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,NA,33,34,35,NA,36,NA,37,38,39,40,41,42,43,NA,44,45,46,47,48)


state_tri_mapper=data.frame(names=state_names,trinom_num=tris) %>% dplyr::mutate(trinom_num = as.character(trinom_num))

state_tri_mapper = state_tri_mapper %>% dplyr::mutate(trinom_num = case_when(names=="Arizona" ~ "AZ", names=="California" ~ "CA", names=="New Mexico" ~ "LA",names=="Maine" ~ "ME",
         TRUE ~ trinom_num))


siteid=c("junk")
sitename=c("junk")
state=c("junk")
collectionunitid=c("junk")
geography=("junk")
potentials = data.frame(siteid=siteid,sitename=sitename,state=state,geography=geography, collectionunitid=collectionunitid)
for (num in seq(length(states[[1]]))) {

coordinates_sf = states[num,] %>% st_convex_hull()


coord_json = sf_geojson(coordinates_sf) %>% URLencode()


sites = content(GET(paste0("https://api.neotomadb.org/v2.0/data/sites?loc=",coord_json,"&limit=9999&offset=0")))$data

if (length(sites) > 0) {

idx = 0
for (i in seq(length(sites))) {
  for (j in seq(length(sites[[i]]$collectionunits))) {
    for (k in seq(length(sites[[i]]$collectionunits[[j]]$datasets))) {
    idx = idx + 1
    }
    }
}


sites_mat = matrix(nrow=idx,ncol=11)

idx2 = 0
for (i in seq(length(sites))) {
  for (j in seq(length(sites[[i]]$collectionunits))) {
    for (k in seq(length(sites[[i]]$collectionunits[[j]]$datasets))) {
    idx2 = idx2 + 1
    for (m in seq(5)) {
      if (!is.null(sites[[i]][[m]])) {
        sites_mat[[idx2, m]] = sites[[i]][[m]]
      }
    }
    
     if (!is.null(sites[[i]]$collectionunits[[j]]$handle)) {
        sites_mat[[idx2,6]] = sites[[i]]$collectionunits[[j]]$handle
     }
       if (!is.null(sites[[i]]$collectionunits[[j]]$collectionunit)) {
        sites_mat[[idx2,7]] = sites[[i]]$collectionunits[[j]]$collectionunit
       }
       if (!is.null(sites[[i]]$collectionunits[[j]]$collectionunitid)) {
        sites_mat[[idx2,8]] = sites[[i]]$collectionunits[[j]]$collectionunitid
       }
       if (!is.null(sites[[i]]$collectionunits[[j]]$collectionunittype)) {
        sites_mat[[idx2,9]] = sites[[i]]$collectionunits[[j]]$collectionunittype
       }
       if (!is.null(sites[[i]]$collectionunits[[j]]$dataset[[k]]$datasetid)) {
        sites_mat[[idx2,10]] = sites[[i]]$collectionunits[[j]]$dataset[[k]]$datasetid
       }
       if (!is.null(sites[[i]]$collectionunits[[j]]$dataset[[k]]$datasettype)) {
        sites_mat[[idx2,11]] = sites[[i]]$collectionunits[[j]]$dataset[[k]]$datasettype
       }
    }
  }
}

sites_state_df = as.data.frame(sites_mat)

names(sites_state_df) = c("siteid","sitename","sitedescription","geography","altitude","handle","collectionunit","collectionunitid","collectionunittype","datasetid","datasettype")

numb = state_tri_mapper %>% dplyr::filter(names == states$NAME[[num]]) %>% dplyr::select(trinom_num)

if (states$NAME[[num]] == "Vermont") {
  potentialsites = sites_state_df %>% dplyr::filter(str_detect(sitename, "VT")) %>% dplyr::select(siteid,sitename,geography,collectionunitid) %>% dplyr::mutate(state = states$NAME[[num]])
  potentials = rbind(potentials,potentialsites)
} else if (states$NAME[[num]] == "Rhode Island") {
  potentialsites = sites_state_df %>% dplyr::filter(str_detect(sitename, "RI")) %>% dplyr::select(siteid,sitename,geography,collectionunitid) %>% dplyr::mutate(state = states$NAME[[num]])
  potentials = rbind(potentials,potentialsites)
} else if (states$NAME[[num]] == "Connectictut") {
  potentialsites = sites_state_df %>% dplyr::filter(str_detect(sitename, "CT")| str_detect(sitename, "6")) %>% dplyr::select(siteid,sitename,geography,collectionunitid) %>% dplyr::mutate(state = states$NAME[[num]])
  potentials = rbind(potentials,potentialsites)
} else if (states$NAME[[num]] == "Arizona") {
  potentialsites = sites_state_df %>% dplyr::filter(str_detect(sitename, "[A-Za-z]{1,2}:[0-9]{1,2}")) %>% dplyr::select(siteid,sitename,geography,collectionunitid) %>% dplyr::mutate(state = states$NAME[[num]])
  potentials = rbind(potentials,potentialsites)
} else if (states$NAME[[num]] == "Alaska") {
  potentialsites = sites_state_df %>% dplyr::filter(str_detect(sitename, "[A-Za-z]{3}-[0-9]+")) %>% dplyr::select(siteid,sitename,geography,collectionunitid) %>% dplyr::mutate(state = states$NAME[[num]])
  potentials = rbind(potentials,potentialsites)
} else if (states$NAME[[num]] == "New York") {
  potentialsites = sites_state_df %>% dplyr::filter(str_detect(sitename, "\\d{5}")|
str_detect(sitename,"30")) %>% dplyr::select(siteid,sitename,geography,collectionunitid) %>% dplyr::mutate(state = states$NAME[[num]])
  potentials = rbind(potentials,potentialsites)
} else if (!is.na(numb[[1]])) {
potentialsites = sites_state_df %>% dplyr::filter(str_detect(sitename,as.character(numb[[1]]))) %>% dplyr::select(siteid,sitename,geography,collectionunitid) %>% dplyr::mutate(state = states$NAME[[num]])
potentials = rbind(potentials,potentialsites)
  } else {
  #print(paste0("Null mapper value for ", states$NAME[[num]]))
}}}


potentials = potentials %>% dplyr::filter(sitename != "junk") %>% dplyr::filter(!str_detect(sitename,"Jensen 1998"))  %>% dplyr::filter(!str_detect(sitename,"Riegel 1965")) %>% dplyr::filter(!str_detect(sitename,"Haile 8A")) %>% dplyr::filter(!str_detect(sitename,"Santa Fe River 8A")) %>% distinct() %>% dplyr::filter(!sitename %in% c("Site 66 (Delcourt et al. 1983)","Site 110 (G.M. Peterson 1978)","Site 10 (Mack, Bryant, and Pell 1978)","LRRB31","Site 106 (Swain unpublished)","Anthony Cave [MALB 29]","Baldy Peak Cave [MALB 29]","Solar Energy Generating Station II [1.76.42]","Luz Solar Trough [1.76.34]", "Site 13 (Delcourt et al. 1983)","Site 14 (Delcourt et al. 1983)","Site 66 (Delcourt et al. 1983)","CHUSKA29","CHUS29","Upper Alturas [UO Loc 2424; CAS Loc 36805]","Site 35 (Heusser 1978)","BECL360","Stand 45 (Mack and Bryant 1974]","Law's [MSv100]","Seven Springs [CE101x3]","Rock House [MS201]","Mobile Bay, MB0810-GC20","Spring Canyon [A-41]","Pratt Cave [TMM-41172]","Little Sunday Canyon Local Fauna [V5414]","Avery Ranch [s:5:8]","Big Juniper House [1595]","Badger House [1453]","Mesa Verde Site 875","La Poudre LP5 Site","SNMN5","SNMN15","ANIM5","ANIM35","COMO5","LPLT5","GATC26","Mile 49 Lac la Ronge Highway","Stations 49-55 (Lichti-Federovich and Ritchie 1968)","TK-49","49 [HFL49]","Valkenswaard N49","Mosquito Lake Site, [PSM-00049]","BOREA24, Mile 49","LR49","ALTAK49","ABD249","Island 35 [10-Q-7]","Hancock Ditch (ditch #39)","Tunica Bayou Site 22","NLA06608-1232","JHMN24 (McAndrews and Wright 1969)","NLA06608-0439","Twin Peaks, AZ","Tucson Mountains, Pima Co., AZ","Vijdt Polder (B49D0349)","Borteldonk (B49F0399)","Halsteren (B49B0566)","Ossendrecht 49D20-1","Shangri-La Bog","Oskino-09","'White-2 (Le4) Pond'","Mud Pond (VT)")) %>%
  dplyr::filter(!str_detect(sitename,"Mount Evans Surface")) %>%  dplyr::filter(!str_detect(sitename,"Elliot-Fisk")) %>%  dplyr::filter(!str_detect(sitename,"Raux-Feui")) %>% dplyr::filter(!str_detect(sitename,"Exposure")) %>%
  dplyr::filter(!str_detect(sitename,"LDDsite")) %>%  dplyr::filter(!str_detect(sitename,"Pond,")) %>%
  dplyr::filter(!str_detect(sitename,"Begin boardwalk")) %>% dplyr::filter(!str_detect(sitename,"unpublished")) %>%
  dplyr::filter(!str_detect(sitename,"Colonel's Island Site")) %>%
  dplyr::filter(!str_detect(sitename,"Delcourt")) %>% dplyr::filter((state !="California") | state == "California" & str_detect(sitename,"CA"))


us_bounds = read_sf("cb_2018_us_nation_20m.shp")


unique_pot = geojson_sf(potentials$geography) %>% cbind(potentials) %>% distinct(siteid,.keep_all=TRUE) %>% st_transform(crs=st_crs(us_bounds)) %>% st_filter(us_bounds)


unique_pot = unique_pot %>% left_join(datasets_df,by=join_by("collectionunitid")) %>% left_join(datasetdatabases_df,by=join_by("datasetid")) %>% left_join(constituentdatabases_df, by=join_by("databaseid"))

faun_smiths = unique_pot %>% dplyr::filter(databasename =="FAUNMAP") %>% distinct(sitename,.keep_all=TRUE)

num_smiths = length(faun_smiths[[1]])
```


# Archaeological Sites

We search for archaeological sites in two ways:

* Smithsonian trinomials in site names, and
* archaeological depositional environments

## Smithsonian Trinomials

Smithsonian trinomials are codes developed by the Smithsonian Institution in the 1930s to uniquely identify archaeological sites which its staff excavated. Later, the duty of minting these trinomials was delegated to the various states. Smithsonian trinomials consist of a 2-digit number indexing a particular US state, a 2-letter abbreviation of the county in which the excavation took place, and a number of arbitrary length to differentiate the site from all others in that county. Because the presence of a Smithsonian trinomial is a convenient way to identify archaeological sites, which are more likely to involve sensitive data, we search here for all the sites in FAUNMAP with Smithsonian trinomials.

The following table offers information on the `r num_smiths` sites in FAUNMAP associated with Smithsonian trinomials, and we map them below the table.
```{r show, echo=FALSE,warning=FALSE,message=FALSE}
datatable(st_drop_geometry(faun_smiths)[c('siteid','sitename','state')],rownames=FALSE)


 pointSites = faun_smiths[st_geometry_type(faun_smiths) == "POINT",] 
    polySites = faun_smiths[st_geometry_type(faun_smiths) == "POLYGON",] 


  leaflet() %>%
  addTiles() %>%
  addPolygons(data = polySites, 
              color = "red", 
              weight = 5, 
              fillColor = "orange", 
              fillOpacity = 0.35, 
              popup = ~sitename)  %>%
  addCircleMarkers(data = pointSites, 
                   radius = 5, 
                   color = "blue", 
                   fillColor = "blue", 
                   fillOpacity = 0.5, 
                   stroke = FALSE, 
                   popup = ~sitename)

  


```


## Depositional environments


```{r depenv, echo=FALSE,warning=FALSE,message=FALSE}



archaeo_faun = faun_sites %>% dplyr::filter(depenvthigherid == "1") %>% distinct(siteid,depenvt,.keep_all = TRUE)


arch_sfc = st_as_sfc(hex2raw(archaeo_faun$geog),EWKB=TRUE)
arch_sf = st_as_sf(archaeo_faun,geom=arch_sfc) %>% dplyr::select(!geog)

num_archaeo_coll = length(archaeo_faun[[1]])

overlap1 = archaeo_faun %>% dplyr::filter(siteid %in% faun_smiths$siteid) %>% distinct()


overlap2 = faun_smiths %>% dplyr::filter(siteid %in% archaeo_faun$siteid) %>% distinct()

num_overlap2 = length(overlap2[[1]])
num_overlap = length(overlap1[[1]])
```


Neotoma associates a depositional environment with every collection unit. Below we display all the sites in FAUNMAP for which at least one collection unit has a depositional environment in the archaeological class. We find `r num_archaeo_coll` such sites, `r num_overlap` of which sites also have Smithsonian trinomials in their name. Conversely, `r num_overlap2` of sites with Smithsonian trinomials have archaeological depositional environments associated with at least one of their collection units in FAUNMAP.

```{r show_arch,echo=FALSE,message=FALSE,warning=FALSE}
datatable(st_drop_geometry(archaeo_faun)[c('siteid','sitename','depenvt')],rownames=FALSE)


 pointSites = arch_sf[st_geometry_type(arch_sf) == "POINT",] 
    polySites = arch_sf[st_geometry_type(arch_sf) == "POLYGON",] 


  leaflet() %>%
  addTiles() %>%
  addPolygons(data = polySites, 
              color = "red", 
              weight = 5, 
              fillColor = "orange", 
              fillOpacity = 0.35, 
              popup = ~sitename)  %>%
  addCircleMarkers(data = pointSites, 
                   radius = 5, 
                   color = "blue", 
                   fillColor = "blue", 
                   fillOpacity = 0.5, 
                   stroke = FALSE, 
                   popup = ~sitename)

  
dictionary = c("human","Human","Homo","homo","indian","Indian","native","Native","indigenous","Indigenous","mound","Mound","buri","Buri","bury","Bury","archaeo","Archaeo","person","Person","people","People","cairn","Cairn")

dictionary = as.data.frame(dictionary)
dictionary <- dictionary[order(dictionary$dictionary),]


matches_sitename <- grep(paste(dictionary,collapse="|"), 
                        faun_sites$sitename)


sitename_results= faun_sites[matches_sitename,] %>% distinct(siteid,.keep_all = TRUE)
num_sacredsites = length(sitename_results[[1]])

```



# Culturally sensitive Areas

We search for any sites whose site or collection unit metadata indicates that it is a culturally sensitive area. We search

* the site name field in sites,
* the location field in collection units, and
* the notes field in collection units,

for the occurrence of any of the following list of terms:

```{r show-dict,message=FALSE,warning=FALSE,echo=FALSE}

datatable(as.data.frame(dictionary),rownames=FALSE)

```

## Site Metadata

We find `r num_sacredsites` sites with a hit for one of our dictionary terms in their names:

```{r sacredsitesareas,message=FALSE,warning=FALSE,echo=FALSE}





datatable(sitename_results[c(1,2)],rownames=FALSE) 
```


## Collection unit metadata

No collection units were found that had any of the dictionary terms in their notes or location fields.

```{r collunits_sacred,echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

faun_colls = collectionunits_df %>% left_join(datasets_df,by=join_by(collectionunitid)) %>% left_join(datasetdatabases_df,by=join_by(datasetid)) %>% left_join(constituentdatabases_df,by=join_by(databaseid)) %>% dplyr::filter(databaseid=='10') %>% distinct(collectionunitid,.keep_all=TRUE)


matches_collunits_location <- grep(paste(dictionary,collapse="|"), 
                        faun_colls$location)


collunits_results= faun_colls[matches_collunits_location,]


## collunits notes

matches_collunits_notes <- grep(paste(dictionary,collapse="|"), 
                        faun_colls$notes)

collunits_results = rbind(collunits_results,faun_colls[matches_collunits_notes,]) %>% distinct()


dictionary = as.data.frame(dictionary)
dictionary <- dictionary[order(dictionary$dictionary),]

datatable(collunits_results[c(1,3,6,7,17,18,20,21)],rownames=FALSE) 



sites = content(GET(paste0("https://api.neotomadb.org/v1.5/data/sites/",paste0(collunits_results$siteid,collapse=","))))$data





site_mat = matrix(nrow=length(sites),ncol=length(sites[[1]]))
for (i in seq(length(sites))) {
  for (j in seq(length(sites[[1]]))) {
    if(!is.null(sites[[i]][[j]])) {
      site_mat[[i,j]] = sites[[i]][[j]]
    }
  }
}

site_df = as.data.frame(site_mat)
if (length(site_df) != 0) {

names(site_df) = c("siteid","sitename","sitedescription","geography","altitude","collectionunitid","collectionunit","handle","unittype","datasetid","datasettype")

site_df = site_df %>% dplyr::filter(collectionunitid %in% collunits_results$collectionunitid) 
datasets = site_df %>% dplyr::filter(datasettype != 'geochronologic') %>% dplyr::select(datasetid) %>% distinct()

ds_list = list()
for (i in seq(14)) {
  val = 25*i
  val0 = val - 24
  ds_list = append(ds_list,content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/",paste0(datasets[val0:val,],collapse=","))))$data)
}

summer=0
for (i in seq(length(ds_list))) {
  summer = summer + length(ds_list[[i]]$site$datasets)
}

ds_mat2 = matrix(nrow=summer,ncol=3)


idx= 0
if (length(ds_list) != 0) {
for (i in seq(length(ds_list))) {
  for (j in seq(length(ds_list[[i]]$site$datasets))) {
    idx = idx + 1
    if (!is.null(ds_list[[i]]$site$datasets[[j]]$datasetid)) {
  ds_mat2[idx,1] = ds_list[[i]]$site$datasets[[j]]$datasetid
    }
      if (!is.null(ds_list[[i]]$site$datasets[[j]]$datasettype)) {
  ds_mat2[idx,2] = ds_list[[i]]$site$datasets[[j]]$datasettype
      }
      if (!is.null(ds_list[[i]]$site$datasets[[j]]$database)) {
  ds_mat2[idx,3] = ds_list[[i]]$site$datasets[[j]]$database
      }
    }
}
ds_df2 = as.data.frame(ds_mat2)


names(ds_df2) = c("datasetid","datasettype","database")

joiner = left_join(site_df,ds_df2) %>% dplyr::select(collectionunitid,datasetid,datasettype,database)

coll_joined = collunits_results %>% dplyr::left_join(joiner) %>% dplyr::filter(datasettype != "geochronologic") %>% group_by(database,sensitivity) %>% count() %>% arrange(desc(n)) %>% drop_na(database)


datatable(coll_joined,rownames=FALSE)}
}
```



# Fuzzed sites

## Original FAUNMAP fuzzing

The original FAUNMAP database contained information that Neotoma no longer maintains on the precision with which its sites' coordinates were delimited. FAUNMAP's original authors write: 

"Site location was established in a variety of ways. In many cases, precise locations (quadrangle, township and range, or latitude and longitude) were actually given in the literature. For all sites with radiocarbon dates that weer reported in the journal <i>Radiocarbon</i>, the latitude and longitude of the site were reported with the dates. In some cases, the reference may not have listed the precise location of the site, but it might have identified the quadrangle that contained the site. If the precise location was not given in the literature, then an approximate location was determined by the person coding the site based upon descriptions of the site location in the reference (e.g., the site is located five miles west of Springfield, IL). These descriptions were then transferred to index maps for the 48 states, provided by the United States Geological Survey (USGS), showing the location of all quadrangles. These maps were used to determine the quadrangle in which the site was located (e.g., the site five miles west of Springfield, IL, is in the Farmingdale Quadrangle). After determining the quadrangle, the latitude and longitude were taken from a USGS list giving the coordinates for the southeast corners of all USGS quadranges (e.g., the longitude and latitude for the southeast corner of the Farmingdale Quadrangle are 89&deg; 45' W and 39&deg; 45'N, respectively). These coordinates were used as the location for the site in the FAUNMAP database (e.g., the location of the site five miles west of Springfield, Illinois, is 89&deg; 45' W longitude and 39&deg; 45'N latitude)."


```{r pressure, echo=FALSE,message=FALSE,warning=FALSE}


faun_sites_nonull = faun_sites %>% dplyr::filter(!is.null(geog)) %>% dplyr::filter(!is.na(geog))
faun_sfc = st_as_sfc(hex2raw(faun_sites_nonull$geog),EWKB=TRUE) 
faunsites_sf = st_as_sf(faun_sites_nonull,geom=faun_sfc) %>% dplyr::select(!geog)

precision_df <- data.frame(
  Code = c("E", "QA", "QP", "QC", "T"),
  Description = c(
    "Exact (lat/long)",
    "Quadrangle Approximate",
    "Quadrangle Precise",
    "Quad. in center of county",
    "USGS Township Coord."
  )
)


locs = read.csv("LOCALITY.txt",header=FALSE)
recov = read.csv("L_RECOV.txt",header=FALSE)
depo = read.csv("DEPOSIT.txt",header=FALSE)
faun= read.csv("FAUNAL.txt",header=FALSE)

names(faun) = c("locid","analysisunit","taxon","confidence","mni","nisp","context","modifications")
names(depo) = c("locid","analysisunit","recoverymethod","depsys","depenv","facies","mixing")

names(locs) = c("locid","sitename_f","alternate_name","state/province","county","sitenumber","coded_by","recdatecreated","recdatemodified","notes","geographic_precision","quadrangle","quadrangle_size","lat","lon","altitude","repository")


sd_faun = locs %>% dplyr::filter(`state/province` == "SD") %>% left_join(depo)

#%>% dplyr::mutate(faun_name = str_replace(sitename,"\\s+\\S+$", "")) 
faun_sites_nonull = faun_sites_nonull %>% dplyr::mutate(faun_name = str_replace(sitename, "\\s*\\[.*$", ""))

faun_prec = locs %>% left_join(faun_sites_nonull,by=join_by("sitename_f" == "faun_name")) %>% dplyr::select(siteid, sitename,sitename_f,lat,lon,geographic_precision, sitenumber) %>% distinct(siteid,.keep_all = TRUE) %>% left_join(precision_df,by=join_by("geographic_precision" == "Code")) %>% dplyr::filter(!is.na(sitename)) %>% dplyr::select(!geographic_precision) %>% group_by(sitename_f) %>% dplyr::filter(sitenumber == str_extract(sitename, "(?<=\\[).*?(?=\\])") | n() == 1 )

faun_points = faunsites_sf[st_geometry_type(faunsites_sf) == "POINT",] %>% st_coordinates() %>% cbind(faunsites_sf[st_geometry_type(faunsites_sf) == "POINT",]) %>% dplyr::mutate(faun_name = str_replace(sitename, "\\s*\\[.*$", "")) %>% left_join(locs,by=join_by("faun_name" == "sitename_f")) %>% dplyr::filter(!is.na(sitename)) %>% dplyr::select(siteid,sitename,faun_name,X,lon, Y,lat) %>% distinct(siteid,.keep_all=TRUE) %>% dplyr::filter(faun_name %in% locs$sitename_f)

names(faun_points) = c("siteid","sitename","faun_name","neo_lon","faun_lon","neo_lat","faun_lat", "geom")


length_orig = length(locs[[1]])
length_prec = length(faun_prec[[1]])
length_points = length(faun_points[[1]])


fuzz_overview = faun_prec %>% group_by(Description) %>% count() %>% arrange(desc(n))

```



"'Exact location' indicates that the actual latitude and longitude of the site were used in determining its location. 'Quadrangle Approximate' was coded if the person coding had to determine the quadrangle based upon a locational description in the reference, as described above. 'Quadrangle Precise' was used if the quadrangle was named in the reference or if township and ranges were given. 'Quadrangle in the center of the county' was used if the county was known but exactly location within the county was not known. In these cases, the latitude and longitude of the quadrangle in the center of the county were coded as the location for the site."


In the table below, we matched `r length_prec` of the original `r length_orig` FAUNMAP sites to their corresponding site in Neotoma and documented the precision originally associated with them.

```{r show_origi_prec, message=FALSE,warning=FALSE,echo=FALSE}


datatable(faun_prec,rownames=FALSE)

original_smiths = faun_prec %>% dplyr::filter(sitename %in% faun_smiths$sitename) 

original_smiths = length(original_smiths[[1]])

original_smiths_overview = faun_prec %>% dplyr::filter(sitename %in% faun_smiths$sitename) %>% group_by(Description) %>% count() %>% arrange(desc(n))


```

Next we show an overview of fuzzed site numbers in the original FAUNMAP database.

```{r counter, echo=FALSE,message=FALSE,warning=FALSE}
datatable(fuzz_overview,rownames=FALSE)

```
`r original_smiths` of these sites have Smithsonian trinomials associated with them. Here is an overview of the precision of those sites:

```{r original_smiths, echo=FALSE,message=FALSE,warning=FALSE}


datatable(original_smiths_overview, rownames=FALSE)


faun_points = faun_points %>% dplyr::mutate(faun_lon = as.character(faun_lon),faun_lat = as.character(faun_lat))

faun_points2 = faun_points %>% dplyr::mutate(faun_lon_deg = case_when(
  str_length(faun_lon) == 7 ~ as.numeric(str_sub(faun_lon,1,3)),
  str_length(faun_lon) == 6 ~ as.numeric(str_sub(faun_lon,1,2))
)) %>% 
  dplyr::mutate(faun_lon_min = case_when(
  str_length(faun_lon) == 7 ~ as.numeric(str_sub(faun_lon,4,5)),
  str_length(faun_lon) == 6 ~ as.numeric(str_sub(faun_lon,3,4))
)) %>% 
  dplyr::mutate(faun_lon_sec = case_when(
  str_length(faun_lon) == 7 ~ as.numeric(str_sub(faun_lon,6,7)),
  str_length(faun_lon) == 6 ~ as.numeric(str_sub(faun_lon,5,6))
)) %>%
  dplyr::mutate(faun_lat_deg = as.numeric(str_sub(faun_lat,1,2)))  %>%
  dplyr::mutate(faun_lat_min = as.numeric(str_sub(faun_lat,3,4))) %>%
  dplyr::mutate(faun_lat_sec = as.numeric(str_sub(faun_lat,5,6)))


faun_points2 = faun_points2 %>% dplyr::mutate(faun_lon_dec = faun_lon_deg + faun_lon_min/60 + faun_lon_sec/3600) %>% dplyr::mutate(faun_lat_dec = faun_lat_deg + faun_lat_min/60 + faun_lat_sec/3600) %>% dplyr::mutate(neo_lon = as.numeric(neo_lon), neo_lat = as.numeric(neo_lat)) %>% dplyr::mutate(neo_lon = abs(neo_lon))

faun_points_discrep = faun_points2 %>% dplyr::filter(abs(neo_lon - faun_lon_dec) > 1e-6 | abs(neo_lat - faun_lat_dec) > 1e-6) %>% select(siteid,sitename,faun_name,neo_lon,faun_lon_dec,neo_lat,faun_lat_dec)



num_discrep = length(faun_points_discrep[[1]])
```


## Site coordinate discrepancies

It is also interesting to note that there are `r num_discrep` sites in the original FAUNMAP database which have different site coordinates now in Neotoma than they did then:

```{r show_origi_loc, message=FALSE,warning=FALSE,echo=FALSE}


datatable(st_drop_geometry(faun_points_discrep),rownames=FALSE)



```

# Human data

We search for any FAUNMAP records in which

* radiocarbon from humans has been measured for the purpose of generating a geochronology, and
* human remains have been counted as a constituent of a faunal assemblage.

## Radiocarbon

Below are the human related radiocarbon samples in FAUNMAP

```{r radio, echo=FALSE,message=FALSE,warning=FALSE}

geochrons = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=geochronology&count=false&limit=99999&offset=0"))$data

geochron_mat = matrix(nrow=length(geochrons),ncol=14)

for (i in seq(length(geochrons))) {
  for(j in seq(14) ) {
    if (!is.null(geochrons[[i]][[j]])) {
  geochron_mat[i,j] = geochrons[[i]][[j]]
    }}}


geochron_df = as.data.frame(geochron_mat)


names(geochron_df) = c("geochronid","sampleid","geochrontypeid","agetypeid","age","errorolder","erroryounger","infinite","delta13c","labnumber","materialdated","notes","recdatecreated","recdatemodified")


matches_geochron_materialDated <- grep(paste(dictionary,collapse="|"), 
                        geochron_df[[11]])

geochron_results= geochron_df[matches_geochron_materialDated,]


samples = list()
for (i in seq(20)) {
  val=i*30000 - 30000
  samples = append(samples,content(GET(paste0('https://api.neotomadb.org/v2.0/data/dbtables/samples?count=false&limit=30000&offset=',val)))$data)
}



sample_mat = matrix(nrow=length(samples),ncol=2)

for (i in seq(length(samples))) {
    if (!is.null(samples[[i]]$sampleid)) {
  sample_mat[i,1] = samples[[i]]$sampleid
    }
      if (!is.null(samples[[i]]$datasetid)) {
  sample_mat[i,2] = samples[[i]]$datasetid
    }}


sample_df = as.data.frame(sample_mat)


names(sample_df) = c("sampleid","datasetid")


sample_filtered = sample_df %>% dplyr::filter(sampleid %in% geochron_results$sampleid) 

sample_topaste = sample_filtered %>% dplyr::select(datasetid) %>% distinct() %>% drop_na()

ds_info = content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/",paste0(sample_topaste[[1]],collapse=","))))$data

ds_mat = matrix(nrow=length(ds_info),ncol=3)


for (i in seq(length(ds_info))) {
    if (!is.null(ds_info[[i]]$site$datasets[[1]]$datasetid)) {
  ds_mat[i,1] = ds_info[[i]]$site$datasets[[1]]$datasetid
    }
      if (!is.null(ds_info[[i]]$site$datasets[[1]]$datasettype)) {
  ds_mat[i,2] = ds_info[[i]]$site$datasets[[1]]$datasettype
      }
      if (!is.null(ds_info[[i]]$site$datasets[[1]]$database)) {
  ds_mat[i,3] = ds_info[[i]]$site$datasets[[1]]$database
    }
}
ds_df = as.data.frame(ds_mat)


names(ds_df) = c("datasetid","datasettype","database")
ds_df = ds_df %>% dplyr::mutate(datasetid = as.numeric(datasetid))
join_df = sample_filtered %>% dplyr::left_join(ds_df)

geochron_results = geochron_results %>% dplyr::mutate(sensitivity =  case_when(
                                                          (grepl("Homo ",materialdated,fixed= TRUE) | grepl("homo",materialdated,fixed= TRUE) | grepl("human",materialdated,fixed= TRUE) | grepl("Human",materialdated,fixed= TRUE)) & (grepl("bone",materialdated,fixed= TRUE) | grepl("Bone",materialdated,fixed= TRUE) | grepl("skull",materialdated,fixed= TRUE) | grepl("Skull",materialdated,fixed= TRUE))
| grepl(21255,geochronid,fixed= TRUE)| grepl(29333,geochronid,fixed= TRUE)| grepl(29334,geochronid,fixed= TRUE)| grepl(29335,geochronid,fixed= TRUE)
 ~ 1,          (grepl("Homo ",materialdated,fixed= TRUE) | grepl("homo",materialdated,fixed= TRUE) | grepl("human",materialdated,fixed= TRUE) | grepl("Human",materialdated,fixed= TRUE)) & (grepl("dung",materialdated,fixed= TRUE) | grepl("Dung",materialdated,fixed= TRUE) | grepl("coprolite",materialdated,fixed= TRUE) | grepl("Coprolite",materialdated,fixed= TRUE) | grepl("feces",materialdated,fixed= TRUE) | grepl("Feces",materialdated,fixed= TRUE)) ~ 2, grepl("human burial", materialdated, fixed=TRUE) ~ 2, grepl("human grave",notes,fixed=TRUE) ~ 2,TRUE ~ 3))

join_df = join_df %>% dplyr::mutate(sampleid = as.character(sampleid))
geochron_counter = geochron_results %>% left_join(join_df) %>% group_by(database,sensitivity) %>% dplyr::filter(database=="FAUNMAP") %>% dplyr::filter(materialdated != "bone, Thomomys talpoides")

datatable(geochron_counter[c(1,2,5,6,7,10,11,12,15)],rownames=FALSE)%>% formatStyle(target="row",
  'sensitivity',
  backgroundColor = styleEqual(c(1,2,3), c('rgba(255, 0, 0, 0.5)','rgba(255, 165, 0, 0.5)','rgba(0, 255, 0, 0.5)'),default="white")
)

```

## Samples

We searched for occurrences of any of the following taxa. 

```{r get-taxa,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
## get taxa
taxalist = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=taxa&limit=75000&offset=0"))$data

taxa_df = matrix(nrow=length(taxalist),ncol=14)

for (i in seq(1,length(taxalist))) {
  for (j in seq(1,14)) {
    if (!is.null(taxalist[[i]][[j]])) {
      taxa_df[i,j] = taxalist[[i]][[j]]
    }
  }
}

taxa_df = as.data.frame(taxa_df)

names(taxa_df) = names(taxalist[[1]])

test1 = grep("Homo ",taxa_df$taxonname)

homos = taxa_df[c(grep("Homo ",taxa_df$taxonname),grep("^Homo$", taxa_df$taxonname),grep("^Homini", taxa_df$taxonname)),]

datatable(homos[c(1,3,6,7)],rownames = FALSE)

## relevant indices are 5571, 6261, 6631

##higher taxa indices are 6261, 6262, 5623


## get occurrences from taxon id
homolist = content(GET("https://api.neotomadb.org/v2.0/data/taxa/6116,6821,6822,7196/occurrences?limit=2500&offset=0"))$data

```


The map below shows the sites where human samples come from, and the table displays some sample-level metadata about them. Rows colored red are sensitivity level 1 because they come from North America. 

It should be noted that lead FAUNMAP steward Jessica Blois has removed all sample-level Homo sapiens occurrences from public access as FAUNMAP works on a policy for managing these data, so what is shown here is actually a subset of the full extent of human records in FAUNMAP.

```{r occurrences, echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}

homo_df = matrix(nrow=length(homolist),ncol=15)

for (i in seq(1,length(homolist))) {
  for (j in seq(1,1)) {
    if (!is.null(homolist[[i]][[j]])) {
      homo_df[i,j] = homolist[[i]][[j]]
    }
    
  }
   for (k in seq(1,4)) {
    if (!is.null(homolist[[i]][[2]][[k]])) {
      homo_df[i,(1+k)] = homolist[[i]][[2]][[k]]
    }}
   for (k in seq(1,3)) {
    if (!is.null(homolist[[i]][[3]][[k]])) {
      homo_df[i,(5+k)] = homolist[[i]][[3]][[k]]
    } }
  
   for (k in seq(1,7)) {
    if (!is.null(homolist[[i]][[4]][[k]])) {
      homo_df[i,(8+k)] = homolist[[i]][[4]][[k]]
    }}
}

homo_df = as.data.frame(homo_df)

names(homo_df) = c("occid","taxonid","taxonname","value","sampleunits","age","ageolder","ageyounger","datasetid","siteid","sitename","altitude","location","datasettype","database")


homo_df = homo_df %>% dplyr::filter(database=="FAUNMAP")
homo_sites = homo_df %>% dplyr::select(siteid) %>% distinct()

homo_sites_neo = get_sites(c(as.numeric(homo_sites[[1]])))

homo_sites_sf = as.data.frame(homo_sites_neo) %>%
  st_as_sf(coords=c("long","lat"),crs="+proj=longlat +datum=WGS84")

homo_full = left_join(homo_sites_sf,homo_df)

homo_bg = osm.raster(homo_sites_sf)

```

``` {r another, echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

rezes = read_sf("tl_2019_us_aiannh.shp") %>% dplyr::select(NAME)

canada_rezes = geojson_sf('https://proxyinternet.nrcan.gc.ca/arcgis/rest/services/CLSS-SATC/CLSS_Administrative_Boundaries/MapServer/0/query?outFields=*&where=1%3D1&f=geojson')
canada_rezes = canada_rezes %>% dplyr::rename('NAME' = 'adminAreaNameEng') %>% dplyr::select(NAME)
canada_rezes = st_transform(canada_rezes,crs="NAD83")
canada_rezes = canada_rezes %>% dplyr::mutate(NAME = str_to_title(NAME))

aust_ipa1 = geojson_sf("https://gis.environment.gov.au/gispubmap/rest/services/ogc_services/Indigenous_Protected_Areas/FeatureServer/0/query?where=NAME+BETWEEN+%27A%27+AND+%27LZ%27&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=&returnGeometry=true&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&gdbVersion=&historicMoment=&returnDistinctValues=false&returnIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&multipatchOption=xyFootprint&resultOffset=&resultRecordCount=&returnTrueCurves=false&returnExceededLimitFeatures=false&quantizationParameters=&returnCentroid=false&timeReferenceUnknownClient=false&sqlFormat=none&resultType=&featureEncoding=esriDefault&datumTransformation=&f=geojson")

aust_ipa2 = geojson_sf('https://gis.environment.gov.au/gispubmap/rest/services/ogc_services/Indigenous_Protected_Areas/FeatureServer/0/query?where=NAME+BETWEEN+%27LZ%27+AND+%27ZZ%27&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=&returnGeometry=true&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&gdbVersion=&historicMoment=&returnDistinctValues=false&returnIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&multipatchOption=xyFootprint&resultOffset=&resultRecordCount=&returnTrueCurves=false&returnExceededLimitFeatures=false&quantizationParameters=&returnCentroid=false&timeReferenceUnknownClient=false&sqlFormat=none&resultType=&featureEncoding=esriDefault&datumTransformation=&f=geojson')
aust_ipa = aust_ipa1 %>% rbind(aust_ipa2)
aust_ipa = aust_ipa1 %>% dplyr::select(NAME)
aust_ipa = st_transform(aust_ipa,crs="NAD83")

rezes = rezes %>% rbind(canada_rezes) %>% rbind(aust_ipa)

rez2 = rezes %>% dplyr::filter(!NAME %in% c('Murray Lake 4','Nunavut Land Claims Agreement \u2013 Hall Beach Inuit Owned Land','Nunavut Land Claims Agreement \u2013 Bathurst Inlet Inuit Owned Land','Nunavut Land Claims Agreement \u2013 Repulse Bay Inuit Owned Land','Nunavut Land Claims Agreement \u2013 Taloyoak Inuit Owned Lands','Isadore Harry 12','Ochapowace I.r. 71-131','Nunavut Land Claims Agreement \u2013 Arctic Bay Inuit Owned Land'))
test = ms_simplify(rez2)


plotLeaflet(homo_sites_neo) %>%
   addPolygons(data = test,
        stroke = FALSE, fillOpacity = 0.5, smoothFactor = 0.5,
        fillColor = ~colors,
        popup= ~NAME)


homo_table = homo_df[c(1,3,4,5,7,8,9,10,11,15)]



sitestring = paste0(homo_df$siteid,collapse=",")

gps = content(GET(paste0("https://api.neotomadb.org/v2.0/data/sites/",sitestring,"/geopoliticalunits?limit=250&offset=0")))$data

gps_mat = matrix(nrow=52,ncol=6)
idx_sites = 0
for (i in seq(length(gps))) {
  for (j in seq(length(gps[[i]]$geopoliticalunits))) {
    idx_sites = idx_sites + 1
    if (!is.null(gps[[i]]$siteid)) {
    gps_mat[idx_sites,1] = gps[[i]]$siteid}
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[1]])) {
    gps_mat[idx_sites,2] = gps[[i]]$geopoliticalunits[[j]][[1]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[2]])) {
    gps_mat[idx_sites,3] = gps[[i]]$geopoliticalunits[[j]][[2]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[3]])) {
    gps_mat[idx_sites,4] = gps[[i]]$geopoliticalunits[[j]][[3]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[4]])) {
    gps_mat[idx_sites,5] = gps[[i]]$geopoliticalunits[[j]][[4]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[5]])) {
    gps_mat[idx_sites,6] = gps[[i]]$geopoliticalunits[[j]][[5]] }
  }
}


gps_df = as.data.frame(gps_mat)

names(gps_df) = c("siteid","rank","gp_id","gp_name","gp_unit","higher_gp_id")

#datatable(gps_df,rownames=FALSE)

gps_sum1 = gps_df %>% dplyr::filter(rank==1) 

homo_table = left_join(homo_table,gps_sum1,by="siteid") %>% dplyr::rename("country" = "gp_name") %>% dplyr::mutate(country = as.factor(country)) %>% mutate(sensitivity = case_when(country == "United States" | country == "Mexico" | country == "Canada" ~ 1, TRUE ~ 2))

datatable(homo_table[c(2:6,8,9,13,16)],rownames=FALSE)

#%>% formatStyle(target="row",
#  'sensitivity',
#  backgroundColor = styleEqual(c(1), #c('rgba(255,0,0,0.5)'),default="rgba(255, 165, 0, 0.5)")
#)

```
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>