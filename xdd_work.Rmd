---
title: "XDD audit extension"
author: "Nick Hoffman"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: pygment
    keep_md: no
    toc: true
    number_sections: true
    toc_depth: 1
    toc_float: true
    theme: journal
editor_options:
    chunk_output_type: inline
---

<style type="text/css">
h2, h3, h4, h5, h6 {
  counter-reset: section;
}
p {
  font-size:18px;
}

ul {
  font-size:18px;
}

li {
  font-size:18px;
}
table {
   padding: 0;border-collapse: collapse;
   layout: fixed;
   width: 90%; }
table tr {
   border-top: 1px solid #cccccc;
   background-color: white;
   margin: 0;
   padding: 0; }
table tr:nth-child(2n) {
   background-color: #f8f8f8; }
table tr th {
   font-weight: bold;
   border: 1px solid #cccccc;
   margin: 0;
   padding: 6px 13px; }
table tr td {
   border: 1px solid #cccccc;
   margin: 0;
   padding: 6px 13px; }
table tr th :first-child, table tr td :first-child {
   margin-top: 0; }
table tr th :last-child, table tr td :last-child {
   margin-bottom: 0; }
.html-widget {
    margin: auto;
}
</style>

---

# Introduction

As part of Neotoma's participation in the Ethical Open Science for Past Global Change Data RCN, we are [auditing our records](https://hoffmanick.github.io/eos/neoHomo.html) for sensitive data, especially data which are sensitive with respect to the CARE principles.  

The main audit document is based on programmatic searches of Neotoma metadata. These structured metadata are curated by a community of experts, so they're a highly accurate source for making determinations about Neotoma's holdings. However, they are necessarily limited in scope. This page documents our attempt to extend our audit through programmatic searches of the full texts of publications associated with datasets in Neotoma according to Neotoma's metadata. This corpus is more extensive, but it is harder to draw conclusions from our hits because it is unstructured.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r prework, echo = FALSE, message = FALSE, warning = FALSE}
library(neotoma2)
library(DT)
library(sf)
library(tidyverse)
library(httr)
library(jsonlite)
library(tmap)
library(osmdata)
library(rosm)
library(geojsonsf)
library(stringr)
library(leaflet)
library(rmapshaper)


sf_use_s2(FALSE)



pubs = content(GET(paste0('https://api.neotomadb.org/v2.0/data/dbtables/publications?count=false&limit=99999&offset=0')))$data



pub_mat = matrix(nrow=length(pubs),ncol=28)

for (i in seq(length(pubs))) {
  for (j in seq(26)) {
    if (!is.null(pubs[[i]][[j]])) {
      pub_mat[[i,j]] = pubs[[i]][[j]]
    }
  }
   if (!is.null(pubs[[i]][[28]])) {
      pub_mat[[i,27]] = pubs[[i]][[28]]
   }
  if (!is.null(pubs[[i]][[27]]$DOI)) {
     pub_mat[[i,28]] = pubs[[i]][[27]]$DOI
  }
}

pub_df = pub_mat %>% as.data.frame() %>% distinct()

names(pub_df) =c("publicationid","pubtypeid","year","citation","articletitle","journal","volume","issue","pages","citationnumber","doi","booktitle","numvolumes","edition","volumetitle","seriestitle","seriesvolume","publisher","url","city","state","country","originallanguage","notes","datecreated","datemodified","bibtex","crossrefDOI")


pub_with_doi = pub_df %>% dplyr::filter(!is.na(doi))

pub_with_crossrefdoi = pub_df %>% dplyr::filter(!is.na(crossrefDOI))

dois_a = pub_with_doi %>% rbind(pub_with_crossrefdoi) %>% distinct()



dois = dois_a %>% dplyr::filter(!doi  %in% c("10.1073\002pnas.0530193100","10.1073\002pnas.0408315102","10.1002/(SICI)1099-1417(199709/10)12:5<347::AID-JQS319>3.0.CO;2-#","10.1130/0091-7613(1987) 15<837:CRDOTP> 2.0.CO;2","10.1016","10.1111","6020"))


diff = length(dois_a[[1]]) - length(dois[[1]])


#articles=list()
#for (i in seq(length(dois[[1]]))) {
#print(i)
#if (!grepl(" ",dois$doi[[i]])) {
#  val = dois$doi[[i]]
#} else {
#  val <- sub(" .*", "", dois$doi[[i]])
#}
    
#  article= content(GET(paste0("https://xdd.wisc.edu/api/articles?doi=",val)))$success$data

#  if (length(article) != 0) {
#    articles = append(articles,article)
#  }
  
#}

#article= content(GET(paste0("https://xdd.wisc.edu/api/articles?doi=10.15468/39omei")))$success$data


#nodoi = pub_df %>% dplyr::filter(is.na(doi)) %>% dplyr::filter(!grepl("https:",articletitle))

#articles2=list()
#for (i in seq(length(nodoi[[1]]))) {
#print(i)
#title = nodoi$articletitle[[i]]
#article= content(GET(paste0("https://xdd.wisc.edu/api/articles?pubname=",URLencode(#title))))$success$data

#  if (length(article) != 0) {
#    articles2 = append(articles2,article)
#  }
  
#}

#articles_all = articles
#article_mat = matrix(nrow=length(articles_all),ncol=5)
#for ( i in seq(length(articles_all))) {
#  article_mat[[i,1]] = articles_all[[i]][['_gddid']]
#  article_mat[[i,2]] = articles_all[[i]]$title
#  article_mat[[i,3]] = articles_all[[i]]$volume
#  article_mat[[i,4]] = articles_all[[i]]$journal
#  if(!is.null(articles_all[[i]]$year)) {
#  article_mat[[i,5]] = articles_all[[i]]$year}
#}

#article_df = as.data.frame(article_mat)
#names(article_df) = c("id","title","volume","journal","year")

article_df = read.csv("article_df.csv")

dois_percent= round(length(dois[[1]])/length(pub_df[[1]])*100,1)

gddid_percent = round(length(article_df[[1]])/length(pub_df[[1]])*100,1)

```

# Neotoma Publications Indexed by XDD

Neotoma contains `r length(pub_df[[1]])` records. Of these `r length(pub_df[[1]])` records, only `r length(dois_a[[1]])` have DOIs. `r diff` of the DOIs are problematically formatted. Of the `r length(dois[[1]])` (`r dois_percent` %) articles with unproblematic DOIs, I searched for how many had internal document ids (XDD ids), by looping all the DOIs through the API call: ```https://xdd.wisc.edu/api/articles?doi=```. Only `r length(article_df[[1]])`  (`r gddid_percent` % ) publications had an internal ID with XDD. It seems likely <em style="color:red;"> (should verify this later) </em> that only publications with a an internal XDD id are indexed by XDD. However, I conduct the following searches using DOI, not internal XDD ID, just in case...

# CARE exemplary practices

One of the things we're looking for is evidence of researcher behavior that demonstrates compliance with the spirit of the CARE principles. Below I outline searches for 


<ol>
<li>fuzzed sites</li>
<li>permitting, and </li>
<li>collaboration with Indigenous nations.</li>
</ol>

## Fuzzed Sites


I searched through the text of Neotoma publications with unproblematic DOI for any that contained one of the following phrases, suggestive of a potential fuzzing process:

```{r cars, message=FALSE, warning=FALSE,echo=FALSE}


fuzzes = list()
whichfuzz_neo = list()
terms_list = c("sensitive site","geomask","low-resolution map","fuzzed site", "obscured location", "purposeful imprecision","obfuscate", "county-level", "county level","county-scale" ,"county scale", "restricted access", "ethic", "legal", "site protection", "redacted", "sensitive date", "Archaeological Resource Protection Act", "National Historic Preservation Act", "withh", "privacy", "damage", "religious site", "cultural data")
terms_df = as.data.frame(terms_list)

names(terms_df) = c("dictionary")

datatable(terms_df,rownames=FALSE)



#for (i in seq(length(dois[[1]]))) {
#  print(i)
#  for (j in seq(length(terms_list))) {
#    if (!grepl(" ",dois$doi[[i]])) {
#  doi = dois$doi[[i]]
#} else {
#  doi <- sub(" .*", "", dois$doi[[i]])
#}
#  doi = URLencode(doi)
#  term = URLencode(terms_list[[j]])
#  call = content(GET(paste0("https://xdd.wisc.edu/api/snippets?term=",term,"&doi=",doi,"&inclusive&full_results&clean")))$success$data
  
#  if (length(call) == 1) {
#    fuzzes = append(fuzzes,call)
#    whichfuzz_neo = append(whichfuzz_neo,dois$publicationid[i])
#  }
  
#  if (length(call) == 2) {
#    if (call[[1]]$doi == call[[2]]$doi) {
#        fuzzes = append(fuzzes,call[1])
#        whichfuzz_neo = append(whichfuzz_neo,dois$publicationid[i])
#      }
#    }
#  if (length(call) > 1) {
#    print(paste0("at i= ", i, " j = ", j, " call length is ", length(call)))
#  }
#  }
#}

#idx = 0
#for ( i in seq(length(fuzzes))) {

#  for (j in seq(length(fuzzes[[i]]$highlight))) {
#    idx = idx + 1

#  }
#}



#fuzz_mat = matrix(nrow = idx,ncol=11)
#idx = 0
#for ( i in seq(length(fuzzes))) {
#    for (j in seq(length(fuzzes[[i]]$highlight))) {
#    idx = idx + 1
#        if(!is.null(fuzzes[[i]][['_gddid']])) {
#    fuzz_mat[idx,1] = fuzzes[[i]][['_gddid']]}
#        if(!is.null(fuzzes[[i]]$title)) {
#    fuzz_mat[idx,2] = fuzzes[[i]]$title}
#        if(!is.null(fuzzes[[i]]$pubname)) {
#    fuzz_mat[idx,3] = fuzzes[[i]]$pubname}
#        if(!is.null(fuzzes[[i]]$authors)) {
#    fuzz_mat[idx,4] = fuzzes[[i]]$authors}
#        if(!is.null(fuzzes[[i]]$hits)) {
#    fuzz_mat[idx,5] = fuzzes[[i]]$hits}
#        if(!is.null(fuzzes[[i]]$URL)) {
#    fuzz_mat[idx,6] = fuzzes[[i]]$URL}
#        if(!is.null(fuzzes[[i]]$publisher)) {
#    fuzz_mat[idx,7] = fuzzes[[i]]$publisher}
#        if(!is.null(fuzzes[[i]]$doi)) {
#    fuzz_mat[idx,8] = fuzzes[[i]]$doi}
#    if(!is.null(fuzzes[[i]]$coverDate)) {
#      fuzz_mat[idx,9] = fuzzes[[i]]$coverDate}
#        if(!is.null(fuzzes[[i]]$highlight[[j]])) {
#    fuzz_mat[idx,10] = fuzzes[[i]]$highlight[[j]]}
#          if(!is.null(whichfuzz_neo[[i]])) {
#    fuzz_mat[idx,11] = whichfuzz_neo[[i]]}
#    
#  }
#}

#fuzz_df = as.data.frame(fuzz_mat) %>% distinct()

fuzz_df = read.csv('fuzz_df.csv')

names(fuzz_df) = c("gddid","title","pubname","authors","hits","URL","publisher","doi","coverDate","highlight","publicationid")



```


This search returned the following `r length(fuzz_df[[1]])` results:


```{r fuzzdict, echo=FALSE, message=FALSE,warning=FALSE}

datatable(fuzz_df,rownames=FALSE)

```



## Permit, Consultation, Collaboration

### Permit

I also searched for any mention of the following terms suggestive of a permitting process of any kind:

```{r pressure, message=FALSE, warning=FALSE,echo=FALSE}

permits = list()
which_per = list()
terms_list_per = c("permit number","permit" ,"thank", "authorit", "collaboration", "consultation", "approval", "IRB", "IACUC")
terms_df_per = as.data.frame(terms_list_per)

names(terms_df_per) = c("dictionary")

datatable(terms_df_per,rownames=FALSE)





#for (i in seq(length(dois[[1]]))) {
  #print(i)
#  for (j in seq(length(terms_list_per))) {
#    if (!grepl(" ",dois$doi[[i]])) {
#  doi = dois$doi[[i]]
#} else {
#  doi <- sub(" .*", "", dois$doi[[i]])
#}
#  doi = URLencode(doi)
#  term = URLencode(terms_list_per[[j]])
#  if (term=="permit%20") {
#    term = "permit%20%23"
#  }
#  call = content(GET(paste0("https://xdd.wisc.edu/api/snippets?term=",term,"&doi=",doi,"&inclusive&full_results&clean&no_word_stemming")))$success$data
  
 # if (length(call) == 2) {
#    if (call[[1]]$doi == call[[2]]$doi) {
#        permits = append(permits,call[1])
#        which_per = append(which_per,dois$publicationid[i])
#      }
#    }
#  if (length(call) ==1) {
#    permits = append(permits,call)
#    which_per = append(which_per,dois$publicationid[i])
#    if (length(call) > 1) {
#      print(paste0("length of call is ", length(call), " at i= ", i, " j = ", j))
 #   }

#  }
#  }
#}

#idx = 0
#for ( i in seq(length(permits))) {

#  for (j in seq(length(permits[[i]]$highlight))) {
 #   idx = idx + 1

#  }
#}



#per_mat = matrix(nrow = idx,ncol=11)
#idx = 0
#for ( i in seq(length(permits))) {

#  for (j in seq(length(permits[[i]]$highlight))) {
#    idx = idx + 1
#        if(!is.null(permits[[i]][['_gddid']])) {
#    per_mat[idx,1] = permits[[i]][['_gddid']]}
#        if(!is.null(permits[[i]]$title)) {
#    per_mat[idx,2] = permits[[i]]$title}
#        if(!is.null(permits[[i]]$pubname)) {
#    per_mat[idx,3] = permits[[i]]$pubname}
#        if(!is.null(permits[[i]]$authors)) {
#    per_mat[idx,4] = permits[[i]]$authors}
#        if(!is.null(permits[[i]]$hits)) {
 #   per_mat[idx,5] = permits[[i]]$hits}
###        if(!is.null(permits[[i]]$URL)) {
#    per_mat[idx,6] = permits[[i]]$URL}
#        if(!is.null(permits[[i]]$publisher)) {
#    per_mat[idx,7] = permits[[i]]$publisher}
#        if(!is.null(permits[[i]]$doi)) {
#    #per_mat[idx,8] = permits[[i]]$doi}
   # if(!is.null(permits[[i]]$coverDate)) {
  #    per_mat[idx,9] = permits[[i]]$coverDate}
 #       if(!is.null(permits[[i]]$highlight[[j]])) {
 #   per_mat[idx,10] = permits[[i]]$highlight[[j]]}
#         if(!is.null(which_per[[i]])) {
#    per_mat[idx,11] = which_per[[i]]}
    
#  }
#}

#per_df = as.data.frame(per_mat) %>% distinct()

#names(per_df) = c("gddid","title","pubname","authors","hits","URL","publisher","doi","coverDate","highlight","publicationid")


per_df = read.csv("per_df.csv")

```

It returned the following `r length(per_df[[1]])` results. However, one issue here is that I'm having trouble searching for the symbol "#". When I use the URL encode function, the transformation to "%23" doesn't happen for some reason. So I manually transform the url to reflect that encoding, but the API doesn't understand what I'm asking for. Or I don't understand how to ask the API for what I want.

``` {r permits, message=FALSE,echo=FALSE,warning=FALSE}

datatable(per_df, rownames=FALSE)



tribes = content(GET('https://cdxapi.epa.gov/oms-tribes-rest-services/api/v1/tribes?tribeNameQualifier=contains&tribalBandFilter=ExcludeTribalBands'))

tribe_mat = matrix(nrow=length(tribes),ncol=2)
for (i in seq(length(tribes))) {
  tribe_mat[[i,1]]=i
  tribe_mat[[i,2]] = tribes[[i]]$currentName
  
}

tribe_df = as.data.frame(tribe_mat)
```

### Federally Recognized Tribe Official Names

Naming a federally recognized tribe would be suggestive of a collaboration between researchers and the tribe named. I searched all Neotoma publications with unproblematic DOIs for any mention of the following official names of federally recognized Indigenous nations. This took a long time (`r length(tribe_df[[1]])` * `r length(dois[[1]])` calls), so I stored the results and will just call those up here. 

```{r termfield, message=FALSE,echo=FALSE,warning=FALSE}

names(tribe_df) = c("index","tribeName")

#tribes_x = list()
#which_pub = list()
#which_trib= list()
terms_list_trib = tribe_df$tribeName
terms_df_trib = as.data.frame(terms_list_trib)


names(terms_df_trib) = c("dictionary")

datatable(terms_df_trib,rownames=FALSE)


#for (i in seq(2200,3132)) {
#  print(i)
#  for (j in seq(length(terms_list_trib))) {
#    if (!grepl(" ",dois$doi[[i]])) {
#  doi = dois$doi[[i]]
#} else {
#  doi <- sub(" .*", "", dois$doi[[i]])
#}
#  doi = URLencode(doi)
#  term = URLencode(terms_list_trib[[j]])

#  call = content(GET(paste0("https://xdd.wisc.edu/api/snippets?term=",term,"&doi=",doi,"&inclusive&full_results&clean")))$success$data
  
#  if (length(call) != 0) {
#    tribes_x = append(tribes_x,call)
#    print(paste0("call length: ",length(call), " for ", i," ",j))
#    which_pub = append(which_pub,dois$publicationid[i])
#    which_trib = append(which_trib,tribe_df$tribeName[[j]])
#  }
#    if (i %in% c(100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500)& j == 1) {
#    print(paste0("at i = ",i, " at ", Sys.time()))
#  }
#  }
#}

## 246,247,248,185, 123
## writing to file tribes_x, which_pub at i=900
##
##

#idx = 0
#for ( i in seq(length(tribes_x))) {

#  for (j in seq(length(tribes_x[[i]]$highlight))) {
#    idx = idx + 1

#  }
#}



#trib_mat = matrix(nrow = idx,ncol=11)
#idx2 = 0
#for ( i in seq(length(tribes_x))) {


 # for (j in seq(length(tribes_x[[i]]$highlight))) {
 #   idx2 = idx2 + 1
#        if(!is.null(tribes_x[[i]][['_gddid']])) {
#    trib_mat[idx2,1] = tribes_x[[i]][['_gddid']]}
#        if(!is.null(tribes_x[[i]]$title)) {
#    trib_mat[idx2,2] = tribes_x[[i]]$title}
#        if(!is.null(tribes_x[[i]]$pubname)) {
#    trib_mat[idx2,3] = tribes_x[[i]]$pubname}
#        if(!is.null(tribes_x[[i]]$authors)) {
#    trib_mat[idx2,4] = tribes_x[[i]]$authors}
#        if(!is.null(tribes_x[[i]]$hits)) {
#    trib_mat[idx2,5] = tribes_x[[i]]$hits}
##        if(!is.null(tribes_x[[i]]$URL)) {
#    trib_mat[idx2,6] = tribes_x[[i]]$URL}
#        if(!is.null(tribes_x[[i]]$publisher)) {
#    trib_mat[idx2,7] = tribes_x[[i]]$publisher}
#        if(!is.null(tribes_x[[i]]$doi)) {
#    trib_mat[idx2,8] = tribes_x[[i]]$doi}
#    if(!is.null(tribes_x[[i]]$coverDate)) {
#      trib_mat[idx2,9] = tribes_x[[i]]$coverDate}
#        if(!is.null(tribes_x[[i]]$highlight[[j]])) {
##    trib_mat[idx2,10] = tribes_x[[i]]$highlight[[j]]}
#         if(!is.null(which_pub[[i]])) {
#    trib_mat[idx2,11] = which_pub[[i]]}
    
#  }
#}

#trib_df = as.data.frame(trib_mat) %>% distinct()

#names(trib_df) = c("gddid","title","pubname","authors","hits","URL","publisher","doi","coverDate","highlight","publicationid")

#write.csv(trib_df,"trib_df2200_3132.csv",row.names=FALSE)

trib_df1 = read.csv("trib_df1_900.csv")
trib_df2 = read.csv("trib_df900_1400.csv")
trib_df3 = read.csv("trib_df1400_2200.csv")
trib_df4 = read.csv("trib_df2200_3132.csv")

trib_df = rbind(trib_df1,trib_df2) %>% rbind(trib_df3) %>% rbind(trib_df4) %>% distinct()

datatable(trib_df, rownames=FALSE)


```


I had a similar problem as I had above with special symbols. The ampersand in Sac & Fox wasn't reading correctly, for instance.


# Radiocarbon from Human Samples


``` {r premio, echo=FALSE,warning=FALSE,message=FALSE}


#geochrons prep
geochrons = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=geochronology&count=false&limit=99999&offset=0"))$data

geochron_mat = matrix(nrow=length(geochrons),ncol=14)

for (i in seq(length(geochrons))) {
  for(j in seq(14) ) {
    if (!is.null(geochrons[[i]][[j]])) {
  geochron_mat[i,j] = geochrons[[i]][[j]]
    }}}


geochron_df = as.data.frame(geochron_mat)


names(geochron_df) = c("geochronid","sampleid","geochrontypeid","agetypeid","age","errorolder","erroryounger","infinite","delta13c","labnumber","materialdated","notes","recdatecreated","recdatemodified")

#collunits prep
collunits = content(GET("https://api.neotomadb.org/v2.0/data/dbtables/collectionunits?count=false&limit=99999"))$data


collunits_mat = matrix(nrow=length(collunits),ncol=20)

for (i in seq(length(collunits))) {
  for(j in seq(20) ) {
    if (!is.null(collunits[[i]][[j]])) {
  collunits_mat[i,j] = collunits[[i]][[j]]
    }}}


collunits_df = as.data.frame(collunits_mat)


names(collunits_df) = c("collectionunitid","handle","siteid","colltypeid","depenvtid","collunitname","colldate","colldevice","gpslatitude","gpslongitude","gpsaltitude","gpserror","waterdepth","substrateid","slopeaspect","slopeangle","location","notes","recdatecreated","recdatemodified")



chronpubs = content(GET('https://api.neotomadb.org/v2.0/data/dbtables/geochronpublications?count=false&limit=99999&offset=0'))$data


chronpub_mat = matrix(nrow=length(chronpubs),ncol=4)

for (i in seq(length(chronpubs))) {
  for(j in seq(4) ) {
    if (!is.null(chronpubs[[i]][[j]])) {
  chronpub_mat[i,j] = chronpubs[[i]][[j]]
    }}}


chronpub_df = as.data.frame(chronpub_mat)


names(chronpub_df) = c("geochronid","publicationid","recdatecreated","recdatemodified")


radiocarbon = content(GET('https://api.neotomadb.org/v2.0/data/dbtables/radiocarbon?count=false&limit=99999&offset=0'))$data


rc_mat = matrix(nrow=length(radiocarbon),ncol=10)

for (i in seq(length(radiocarbon))) {
  if (!is.null(radiocarbon[[i]]$geochronid[[1]])) {
    rc_mat[i,1] = radiocarbon[[i]]$geochronid[[1]]
  }
    if (!is.null(radiocarbon[[i]]$radiocarbonmethodid[[1]])) {
    rc_mat[i,2] = radiocarbon[[i]]$radiocarbonmethodid[[1]]
  }
  for(j in seq(6) ) {
    if (!is.null(radiocarbon[[i]][[(j+2)]])) {
  rc_mat[i,(j+2)] = radiocarbon[[i]][[(j+2)]]
    }}
   for(j in seq(2) ) {
    if (!is.null(radiocarbon[[i]][[(j+8)]])) {
  rc_mat[i,(j+8)] = radiocarbon[[i]][[(j+8)]]
    }}}


rc_df = as.data.frame(rc_mat)


names(rc_df) = c("geochronid","radiocarbonmethodid","percentc","percentn","delta13c","delta15n","percentcollagen","reservoir","masscmg","cnratio")



#
materialtypes = geochron_df %>% dplyr::group_by(materialdated) %>% count()


ambig_rad = geochron_df %>% filter(materialdated %in% c("Bone collagen","bone collagen","","bone","Bone") | is.na(materialdated))


ambig_rad_pubs = chronpub_df %>% dplyr::filter(geochronid %in% ambig_rad$geochronid) %>% distinct()

ambig_rad_pubs = pub_df %>% dplyr::filter(publicationid %in% ambig_rad_pubs$publicationid)


ambig_rad_pubsd = ambig_rad_pubs %>% dplyr::filter(!is.na(doi)) %>% dplyr::filter(!doi %in% c("10.1016"))


articles_ambig=list()


pubids = list()
#for (i in seq(length(ambig_rad_pubsd[[1]]))) {
  #print(i)
#if (!grepl(" ",ambig_rad_pubsd$doi[[i]])) {
#  val = ambig_rad_pubs$doi[[i]]
#} else {
#  val <- sub(" .*", "", ambig_rad_pubsd$doi[[i]])
#}
#  if (ambig_rad_pubsd$doi[[i]] == "10.1016") {
#  val = "nevermind"
#}
    
  #article= content(GET(paste0("https://xdd.wisc.edu/api/articles?doi=",val)))$success$data

  #if (length(article) != 0) {
  #  articles_ambig = append(articles_ambig,article)
  #  pubids = append(pubids,rep(ambig_rad_pubsd$publicationid[[i]]),length(article))
  #}
  
    #if (length(article) == 10) {
  #    for ( i in seq(10)) {
       # print(val)
    #print(article[[i]][["_gddid"]])
   #     }
 # }
  
  
#}


#article_ambig_mat = matrix(nrow=length(articles_ambig),ncol=6)
#for ( i in seq(length(articles_ambig))) {
#  article_ambig_mat[[i,1]] = articles_ambig[[i]][['_gddid']]
#  article_ambig_mat[[i,2]] = articles_ambig[[i]]$title
#  article_ambig_mat[[i,3]] = articles_ambig[[i]]$volume
#  article_ambig_mat[[i,4]] = articles_ambig[[i]]$journal
#  if(!is.null(articles_ambig[[i]]$year)) {
#  article_ambig_mat[[i,5]] = articles_ambig[[i]]$year}
#  article_ambig_mat[[i,6]] = pubids[[i]]
#}

#article_ambig_df = as.data.frame(article_ambig_mat)
#names(article_ambig_df) = c("id","title","volume","journal","year", "publicationid")

article_ambig_df = read.csv("article_ambig_df.csv")


```

As part of our interest in our repository's relationship to CARE, we want to know about the disposition of the remains of human ancestors, for both radiocarbon dates and samples. There are `r length(geochron_df[[1]])` records in Neotoma's geochronology table, of which there are `r length(ambig_rad[[1]])` records ambiguous with respect to taxon (meaning the <i>materialdated</i> field is bone collagen, bone, blank, or NA). We would like to search for those ambiguous taxa which are most likely to be dated from human material.

We searched for all the publications in Neotoma associated with these ambiguous records and found `r length(ambig_rad_pubs[[1]])` records. Of these records, only `r length(ambig_rad_pubsd[[1]])` have an unproblematic DOI, and `r length(article_ambig_df[[1]])` have an internal XDD ID.

I searched through all the Neotoma publications associated with an ambiguous taxon and with a DOI for the following terms:

``` {r per fed, message=FALSE,echo=FALSE,warning=FALSE}


human_rcs = list()
terms_list_rc = c("human specimen", "human bone")
which_human_pub = list()

terms_list_rc_df = as.data.frame(terms_list_rc)
names(terms_list_rc_df) = c("dictionary")

datatable(terms_list_rc_df, rownames=FALSE)

```



```{r moreambigrad, message=FALSE,echo=FALSE,warning=FALSE}



#for (i in seq(length(ambig_rad_pubsd[[1]]))) {
#  print(i)
#  for (j in seq(length(terms_list_rc))) {
#    if (!grepl(" ",ambig_rad_pubsd$doi[[i]])) {
#      doi = ambig_rad_pubsd$doi[[i]]
#      } else {
#      doi <- sub(" .*", "", ambig_rad_pubsd$doi[[i]])
#  }
#  doi = URLencode(doi)
#  term = URLencode(terms_list_rc[[j]])
#  call = content(GET(paste0("https://xdd.wisc.edu/api/snippets?term=",term,"&doi=",#doi,"&inclusive&full_results&clean")))$success$data
  
 # if (length(call) != 0) {
    #print(paste0(length(call), "at i = ", i))
#    human_rcs = append(human_rcs,call)
#    which_human_pub = append(which_human_pub,ambig_rad_pubsd$publicationid[[i]])
    
 # }
#  }
#}

#idx = 0
#for ( i in seq(length(human_rcs))) {

 # for (j in seq(length(human_rcs[[i]]$highlight))) {
#    idx = idx + 1

 # }
#}



#human_rc_mat = matrix(nrow = idx,ncol=11)
#idx = 0
#for ( i in seq(length(human_rcs))) {

 # for (j in seq(length(human_rcs[[i]]$highlight))) {
#    idx = idx + 1
#        if(!is.null(human_rcs[[i]][['_gddid']])) {
#    human_rc_mat[idx,1] = human_rcs[[i]][['_gddid']]}
#        if(!is.null(human_rcs[[i]]$title)) {
#    human_rc_mat[idx,2] = human_rcs[[i]]$title}
#        if(!is.null(human_rcs[[i]]$pubname)) {
#    human_rc_mat[idx,3] = human_rcs[[i]]$pubname}
#        if(!is.null(human_rcs[[i]]$authors)) {
##    human_rc_mat[idx,4] = human_rcs[[i]]$authors}
#        if(!is.null(human_rcs[[i]]$hits)) {
#    human_rc_mat[idx,5] = human_rcs[[i]]$hits}
#        if(!is.null(human_rcs[[i]]$URL)) {
#    human_rc_mat[idx,6] = human_rcs[[i]]$URL}
#        if(!is.null(human_rcs[[i]]$publisher)) {
##    human_rc_mat[idx,7] = human_rcs[[i]]$publisher}
#        if(!is.null(human_rcs[[i]]$doi)) {
#    human_rc_mat[idx,8] = human_rcs[[i]]$doi}
#    if(!is.null(human_rcs[[i]]$coverDate)) {
#      human_rc_mat[idx,9] = human_rcs[[i]]$coverDate}
#        if(!is.null(human_rcs[[i]]$highlight[[j]])) {
#    human_rc_mat[idx,10] = human_rcs[[i]]$highlight[[j]]}
#            if(!is.null(which_human_pub[[i]])) {
#    human_rc_mat[idx,11] = which_human_pub[[i]]}
    
 # }#
#}

#human_rc_df = as.data.frame(human_rc_mat)


human_rc_df = read.csv("human_rc_df.csv")

names(human_rc_df) = c("gddid","title","pubname","authors","hits","URL","publisher","doi","coverDate","highlight", "publicationid")

```


This time, the DOI search returned `r length(human_rc_df[[1]])` results:

```{r evenmoreambigrad, message=FALSE, echo = FALSE, warning =FALSE}

datatable(human_rc_df,rownames=FALSE)

```

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>