---
title: "DRAFT: The FAUNMAP Constituent Database"

output:
  html_document:
    df_print: paged
    highlight: pygment
    keep_md: yes
    number_sections: no
    theme: journal
editor_options:
    chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(jsonlite)
library(DT)
library(tidyverse)
library(geojsonsf)
library(sf)
library(sfheaders)

library(rosm)
library(osmdata)
library(tmap)
library(leaflet)
library(ggplot2)
library(neotoma2)
library(stringr)
library(stringi)
```


```{r gather-sets-sites, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}




database_list = c("FAUNMAP")


datatypes = c("biochemistry","biomarker","charcoal","charcoal surface sample", "chironomid", "cladocera", "diatom", "diatom bottom sample", "diatom surface sample", "diatom top-bottom", "dinoflagellates", "energy dispersive X-ray spectroscopy (EDS/EDX)", "geochemistry", "geochronologic", "insect", "insect modern", "loss-on-ignition", "macrocharcoal", "macroinvertebrate", "Metabarcoding aeDNA", "microcharcoal", "modern biochemistry", "organic carbon", "ostracode", "ostracode surface sample", "paleomagnetic", "physical sedimentology", "phytolith", "plant macrofossil", "pollen", "pollen surface sample", "pollen trap", "specimen stable isotope", "stable isotope", "testate amoebae", "testate amoebae surface sample", "vertebrate fauna", "water chemistry", "X-ray diffraction (XRD)", "X-ray fluorescence (XRF)")


# For every database (just FAUNMAP), the "returns" variable calls all the datasets associated with the database, organized by site. Then, if the database has datasets/sites associated with it (some don't but FAUNMAP does), do another loop: count the number of datasets. And if the site has a location and site description, grab those values. If the geometry of the location is a polygon, just take the centroid. If there's no site description, don't grab those values. And do another loop: for every site, go through all the datasets and grab the type and id, and increment a datasets index variable since it's not clear a priori how many datasets there are. Then do some other stuff, like make a table (db_stats) which has all the FAUNMAP datasets organized by datatype.

db_stats = matrix(nrow=length(database_list),ncol = 43)
idx_datasets =0
datasets_mat =  matrix(nrow=1000000,ncol=1)
for (i in seq(1,length(database_list))) {
  db_stats[i,1] = database_list[i]
  #print(db_stats[i,1])
  db_format = gsub(" ", '%20', database_list[i], fixed=TRUE)
  returns = content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/db?limit=10000&offset=0&database=",db_format)))$data
  num_sites= length(returns)
  db_stats[i,2] = num_sites
  num_datasets = 0
  setlist=c()
  placeslist = as.data.frame(matrix(ncol=5,nrow=1)) 
  names(placeslist) = c("siteid","sitename","sitedescription","long","lat")
  placeslist$long[1] = 0
  placeslist$lat[1] = 0
  placeslist$siteid[1] = 0
  placeslist$sitename[1] = 0
  placeslist$sitedescription[1] = 0
  placeslist = placeslist %>%
    st_as_sf(coords=c("long","lat"),crs="+proj=longlat +datum=WGS84")
  if (length(returns) != 0) {
    for (j in seq(1,length(returns))) {
      num_datasets = num_datasets + length(returns[[j]]$site$datasets)
      if (!is.null(returns[[j]]$site$geography) & !is.null(returns[[j]]$site$sitedescription)) {
      newplace = geojson_sf(returns[[j]]$site$geography) %>% 
        cbind(as.data.frame(returns[[j]]$site[1:3]))
      if (st_geometry_type(newplace$geometry)=="POLYGON") {
        newplace = st_centroid(newplace) 
      }
      placeslist = rbind(placeslist,newplace)}
      
      if (!is.null(returns[[j]]$site$geography) & is.null(returns[[j]]$site$sitedescription)) {
      newplace = geojson_sf(returns[[j]]$site$geography) %>% 
        cbind(as.data.frame(returns[[j]]$site[1:2])) %>%
        cbind(as.data.frame(""))
      names(newplace) = names(placeslist)
      if (st_geometry_type(newplace$geometry)=="POLYGON" & is.null(returns[[j]]$site$sitedescription)) {
        newplace = st_centroid(newplace)
      names(newplace) = names(placeslist)
      }
      placeslist = rbind(placeslist,newplace)}
      for (k in seq(1,length(returns[[j]]$site$datasets))) {
        setlist = append(setlist,returns[[j]]$site$datasets[[k]]$datasettype)
        datasets_mat[idx_datasets] = returns[[j]]$site$datasets[[k]]$datasetid
        idx_datasets = idx_datasets + 1
      }}
      placeslist = placeslist[2:length(placeslist[[1]]),]
     
      #df = sf_to_df(placeslist) %>%
      #  cbind(placeslist$siteid) %>%
      #  cbind(placeslist$sitename) %>%
      #  cbind(placeslist$sitedescription)

      setlist = as.data.frame(setlist) %>% group_by(setlist) %>% count()
      set_wide = setlist %>% pivot_wider(names_from="setlist",values_from="n")
      #print(set_wide)
      for (l in seq(1,40)) {
        if (datatypes[l] %in% names(set_wide)) {
          place = datatypes[l]
          selector = set_wide %>% select(all_of(place))
          #print(selector)
          db_stats[i,(l+3)] = selector[[1]]
      }
      }
    #print(num_datasets)
    db_stats[i,3] = num_datasets}
  if (length(returns) == 0) {
      db_stats[i,3] = 0
    }
  }
db_stats = as.data.frame(db_stats)
names(db_stats) = c("dbName","numSites","numSets","biochemistry","biomarker","charcoal","charcoal surface sample", "chironomid", "cladocera", "diatom", "diatom bottom sample", "diatom surface sample", "diatom top-bottom", "dinoflagellates", "energy dispersive X-ray spectroscopy (EDS/EDX)", "geochemistry", "geochronologic", "insect", "insect modern", "loss-on-ignition", "macrocharcoal", "macroinvertebrate", "Metabarcoding aeDNA", "microcharcoal", "modern biochemistry", "organic carbon", "ostracode", "ostracode surface sample", "paleomagnetic", "physical sedimentology", "phytolith", "plant macrofossil", "pollen", "pollen surface sample", "pollen trap", "specimen stable isotope", "stable isotope", "testate amoebae", "testate amoebae surface sample", "vertebrate fauna", "water chemistry", "X-ray diffraction (XRD)", "X-ray fluorescence (XRF)")



db_stats = t(db_stats) %>%
  as.data.frame()

db_stats$dataset = row.names(db_stats)
names(db_stats) = c("number","datasets")

db_stats = db_stats %>%
  dplyr::filter(number != 0)

db_stats = db_stats[c(2,1)]

```

FAUNMAP is one of the original Neotoma Constituent Databases. FAUNMAP was originally developed by Russ Graham and Ernest Lundelius in the early 1990s with <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=9005144">funding from the NSF</a>. FAUNMAP initially aggregated mammalian occurrences from across the United States for the past 50,000 years. In the late 1990s, the temporal reach of the database was extended to encompass the past five million years. FAUNMAP does contain some data on vertebrate taxa outside mammals (as well as a few plant and invertebrate datasets!), but mammals are its primary focus.

FAUNMAP currently contains `r db_stats$number[3]` datasets from `r length(unique(placeslist)[[1]])` sites from the locations mapped below.


```{r dataset-api, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

##take the matrix of dataset ids and types from the first loop, just grab the values that are full (I had made a lot more rows than I would need since I wasn't sure how many rows I would need). Then to get the more specific metadata associated with them, run the Neotoma datasets API on the dataset IDs. I couldn't figure out how to get more than 25 at a time so I ran this call 280ish times in groups of 25.

datasets_df = as.data.frame(datasets_mat)
datasets_df = datasets_df %>% dplyr::filter(!is.na(V1))


runs = ceiling(length(datasets_df[[1]]) / 25)

dataset_calls = list()
for (i in seq(runs)) {
  #print(i)
  beg = (i-1)*25
  end = i*25
  sets_list1 = paste0(datasets_df$V1[beg:end],collapse=",")
  dataset_call1 = content(GET(paste0('https://api.neotomadb.org/v2.0/data/datasets/',sets_list1)))$data
  dataset_calls = append(dataset_call1,dataset_calls)
}


#Although there are fewer datasets, I converted the list I got from the above call into a matrix with ~12,000 rows because I wanted to have a distinct row for each PI associated with a dataset.
dataset_info_mat = matrix(ncol = 23, nrow =11957)


idx_dataset_stuff = 0
for (j in seq(1,length(dataset_calls))) {
  
  for (k in seq(1,length(dataset_calls[[j]]$site$datasets))) {
    
    for (m in seq(length(dataset_calls[[j]]$site$datasets[[k]]$datasetpi))) {
      idx_dataset_stuff = idx_dataset_stuff + 1
      
      for (n in seq(10)) {
      if(!is.null(dataset_calls[[j]]$site[[n]])) {
      dataset_info_mat[idx_dataset_stuff,n] = dataset_calls[[j]]$site[[n]]
      }}
      if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$doi[[1]])) {
      dataset_info_mat[idx_dataset_stuff,11] = dataset_calls[[j]]$site$datasets[[k]]$doi[[1]]
      }
      
      
      if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$agerange[[1]][[1]])) {
      dataset_info_mat[idx_dataset_stuff,12] = dataset_calls[[j]]$site$datasets[[k]]$agerange[[1]][[1]]
      }
      
        if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$agerange[[1]][[2]])) {
      dataset_info_mat[idx_dataset_stuff,13] = dataset_calls[[j]]$site$datasets[[k]]$agerange[[1]][[2]]
      }
      
      
        if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$agerange[[1]][[3]])) {
      dataset_info_mat[idx_dataset_stuff,14] = dataset_calls[[j]]$site$datasets[[k]]$agerange[[1]][[3]]
      }
      
      
        if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$database)) {
      dataset_info_mat[idx_dataset_stuff,15] = dataset_calls[[j]]$site$datasets[[k]]$database
        }
      
      if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$datasetid)) {
      dataset_info_mat[idx_dataset_stuff,16] = dataset_calls[[j]]$site$datasets[[k]]$datasetid
      }
      if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$datasettype)) {
      dataset_info_mat[idx_dataset_stuff,22] = dataset_calls[[j]]$site$datasets[[k]]$datasettype
      }
      
        if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$datasetnotes)) {
      dataset_info_mat[idx_dataset_stuff,23] = dataset_calls[[j]]$site$datasets[[k]]$datasetnotes
      }
      
      for (o in seq(5)) {
      if(!is.null(dataset_calls[[j]]$site$datasets[[k]]$datasetpi[[m]][[o]])) {
      dataset_info_mat[idx_dataset_stuff,(16+o)] = dataset_calls[[j]]$site$datasets[[k]]$datasetpi[[m]][[o]]
      }}
    }
    
  }
}



dataset_info_df = as.data.frame(dataset_info_mat)

names(dataset_info_df) = c("siteid","sitename","sitedescription","sitenotes","geography","altitude","collectionunitid","collectionunit","handle","unittype","doi","age_units","ageold","ageyoung","database","datasetid","initials","contactid","firstname","familyname","contactname","datasettype","datasetnotes")

#write.csv(dataset_info_df,"faunmap_dataset_info_df.csv",row.names=FALSE)



## Since I also wanted to know the date that each dataset was created, I ran another call to download the entire datasets table from Neotoma. Again I had a "limited to 25 issue" so I ran the call in groups of 25.

table_calls = list()
for (i in seq(2100)) {
  #print(i)
  off=(i-1)*25
  table_call1 = content(GET(paste0("https://api.neotomadb.org/v2.0/data/dbtables/datasets?limit=25&offset=",off)))$data

  table_calls = append(table_call1,table_calls)
}


table_call_mat = matrix(nrow=length(table_calls),ncol=8)
for (i in seq(length(table_calls))) {
  for (j in seq(1,8)) {
    if (!is.null(table_calls[[i]][[j]])) {
      table_call_mat[i,j] = table_calls[[i]][[j]]
    }
}}


table_call_mat = as.data.frame(table_call_mat)

names(table_call_mat) = c("datasetid","collectionunitid","datasettypeid","datasetname","notes","recdatecreated","recdatemodified","embargoid")


## I combined the two datasets dataframes I just created (by=c("datasetID","collectionunitID"))
datasets_full = left_join(dataset_info_df,table_call_mat)


#write.csv(table_call_mat,"neotoma_datasets_mastertable.csv",row.names=FALSE)

#write.csv(datasets_full,"faunmap_datasets_mastertable.csv",row.names=FALSE)

datasets_full = read.csv("faunmap_datasets_mastertable.csv")

datasets_full = datasets_full %>% mutate(date_create = word(recdatecreated,1,sep="T")) %>% mutate(date_create= ymd(date_create))

#datasets_full_distinct gets rid of duplicate datasets (because of multiple PIs). This will be useful later
datasets_full_distinct = datasets_full %>% distinct(datasetid,.keep_all=TRUE)


```

```{r map, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}


##To actually map the sites in FAUNMAP database, I take the siteIDs I got from the full datasets table, and use the neotoma2 R package get_sites() and plotLeaflet() functions on them.

sites = datasets_full %>% distinct(siteid)
all_sites = get_sites(as.numeric(sites$siteid),all_data=TRUE)
plotLeaflet(all_sites)


```

The temporal extent of the FAUNMAP database is a little bimodal: about half the records have a plio-pleistocene time scale, and the other half have a more holocene time scale. (And only 4815 of the 6950 datasets have any time range associated with them at all.)

```{r temp, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

#To map the temporal extent of the database, I grab just the full dataset records with an associated age range, and I add an index variable to them for plotting. Then I draw line segments representing the age ranges and present the plot with two different time scales because you can see different aspects of the temporal extent better at different time scales.

datasets_full_time = datasets_full_distinct %>% dplyr::filter(!is.na(ageold))

datasets_full_time = datasets_full_time %>% mutate(ageold = as.numeric(ageold)) %>% mutate(ageyoung = as.numeric(ageyoung))  %>% arrange(desc(ageold))
datasets_full_time$index = seq(1,length(datasets_full_time[[1]]))


ggplot(datasets_full_time) +
  geom_segment(mapping=aes(x=as.numeric(ageold),xend=as.numeric(ageyoung),y=index,yend=index),color="darkblue") +
  scale_x_continuous(name="Age Range (Years)",expand=c(0,0)) +
  scale_y_continuous(name="Dataset Index",limits=c(0,length(datasets_full_time[[1]])),expand=c(0,0)) +
  theme_bw() +
  ggtitle("Temporal Extent of FAUNMAP Records: Plio-Pleistocene Scale")


ggplot(datasets_full_time) +
  geom_segment(mapping=aes(x=as.numeric(ageold),xend=as.numeric(ageyoung),y=index,yend=index),color="darkblue") +
  scale_x_continuous(name="Age Range (Years)",expand=c(0,0),limits=c(0,11000)) +
  scale_y_continuous(name="Dataset Index",limits=c(0,length(datasets_full_time[[1]])),expand=c(0,0)) +
  theme_bw() +
  ggtitle("Temporal Extent of FAUNMAP Records: Holocene Scale")

```

Most FAUNMAP datasets are vertebrate fauna or geochronologic, with a few outlying dataset kinds. 

```{r table, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

## Then I plot the db_stats I got from the original loop.

db_stats = db_stats %>% dplyr::filter(!datasets %in% c('dbName','numSites','numSets')) %>% arrange(desc(as.numeric(number)))
datatable(db_stats,rownames=FALSE,options = list(pageLength = 40))

```

In 2018, thousands of new datasets were uploaded to FAUNMAP.

```{r uploads-in-time,  echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

#In order to make a plot that shows cumulative uploads over time, I group distinct datasets by date created, and add a column with the cumulative sum.


list_dates = datasets_full_distinct %>% group_by(date_create) %>% count() %>% arrange(date_create) 
list_dates$cumsum = cumsum(list_dates$n)

ggplot(list_dates) +
    geom_path(mapping=aes(x=date_create,y=cumsum)) +
    theme_bw() +
    ggtitle("Cumulative Dataset Uploads to FAUNMAP") +
    scale_x_date(name="Date Uploaded", expand=c(0,0)) +
    scale_y_continuous(name="Cumulative Uploads")

#Trying to mimic Simon's state of the databse plot about uploads by type by month. Had to add in a bunch of 0 values for year_months where no records were uploaded.

datasets_full_distinct$month_year <- format(as.Date(datasets_full_distinct$date_create), "%Y-%m")

uploads_by_type = datasets_full_distinct  %>% group_by(month_year,datasettype) %>% count()

attested_months = uploads_by_type %>% ungroup() %>% select(month_year) %>% distinct(month_year)

poss_years = seq(2013,2024)
poss_months = c("01","02","03","04","05","06","07","08","09","10","11","12")

poss_dates = matrix(ncol=1,nrow=(12*12))
date_idx=0
for (i in seq(length(poss_years))) {
  for (j in seq(length(poss_months))) {
    y = poss_years[i]
    m = poss_months[j]
    poss_date = paste0(y,"-",m)
    poss_dates[date_idx,1] = poss_date
    date_idx = date_idx + 1
  }
}

poss_dates = as.data.frame(poss_dates)
names(poss_dates) = c("month_year")

unattested_months = poss_dates %>% dplyr::filter(!poss_date %in% uploads_by_type$month_year) %>% drop_na() %>%
  cbind(datasettype = "vertebrate fauna") %>% cbind(n=0)

uploads_by_type = uploads_by_type %>% rbind(unattested_months)

ggplot(uploads_by_type) +
    geom_col(mapping=aes(x=month_year,y=n,fill=datasettype)) +
    theme_bw() +
    ggtitle("Uploads to FAUNMAP by Dataset Type") +
    scale_x_discrete(name="Month and Year", breaks=c("2013-09","2015-09","2017-09","2019-09","2021-09","2023-09"),labels=c("Sep 2013","Sep 2015","Sep 2017","Sep 2019","Sep 2021","Sep 2023")) +
    scale_y_continuous(name="Number of Records Uploaded") +
  scale_fill_viridis_d()
```

Prior to 2018, the researcher associated with the most FAUNMAP datasets was John H. Brumley, who had 29 datasets in the database.

```{r early-up,  echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

#uploaders from before 2018
uploaders_early = datasets_full %>% dplyr::filter(year(date_create) <2018) %>% group_by(contactid,contactname) %>% count() %>% arrange(desc(n)) %>% drop_na()


datatable(uploaders_early,rownames=FALSE)

```

Currently, the researcher with the most data uploaded to FAUNMAP is Charles Richard Harington, who is associated with 259 datasets in the database.

```{r all-time-ups,  echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

#all time top uploaders

uploaders = datasets_full %>% group_by(contactid,contactname) %>% count() %>% arrange(desc(n)) %>% drop_na()

datatable(uploaders,rownames=FALSE)


```

```{r indig-land, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

##two approaches here to thinking about how FAUNMAP sites relate to Indigenous land as recorded by native-land.ca

# approach #1: grab all Indigenous territory polygons with the following single call:
indigTerr = content(GET("https://native-land.ca/wp-content/themes/NLD-2021/files/indigenousTerritories.json"))$features


## to turn the returned list into a dataframe I can use, I grab the name, ID, and slug of each returned Indigenous territory in one matrix (indigTerr_mat), and I grab all the points associated with the geometries for those territories in another matrix (df_mat) which I make really big because I don't know a priori how many total points there will be. I also put the ID for the territory in this second matrix to keep track of where the points go. And I only grab the points associated with territories that have a single polygon associated with them, not multiple polygons.

indigTerr_mat = matrix(nrow=length(indigTerr),ncol=4)
df_mat = matrix(nrow=1000000,ncol=3)
idx=0
for (i in seq(1,length(indigTerr))) {
  if (!is.null(indigTerr[[i]]$properties$Name)) {
    indigTerr_mat[i,1] = indigTerr[[i]]$properties$Name
  }
  if (!is.null(indigTerr[[i]]$properties$ID)) {
    indigTerr_mat[i,2] = indigTerr[[i]]$properties$ID
  }
  if (!is.null(indigTerr[[i]]$properties$Slug)) {
    indigTerr_mat[i,3] = indigTerr[[i]]$properties$Slug
  }

  if (length(indigTerr[[(i)]]$geometry$coordinates[[1]][[1]][[2]]) ==1) {
  for (j in seq(length(indigTerr[[i]]$geometry$coordinates[[1]]))) {
    idx = idx+1
    df_mat[idx,1] = indigTerr[[i]]$geometry$coordinates[[1]][[j]][[2]]
    df_mat[idx,2] = indigTerr[[i]]$geometry$coordinates[[1]][[j]][[1]]
    df_mat[idx,3] = indigTerr[[i]]$properties$ID

  }


  }}

  #I turn all those points into an sf polygon, using the territory ID to keep track of which points belong to which polygon. Then I join the sf object to the indigTerr_mat dataframe, using the territory's ID as the column-in-common.

  
  indigTerr_mat = as.data.frame(indigTerr_mat)[1:3]
  names(indigTerr_mat) = c("name","id","slug")
  
  indigTerr_mat = indigTerr_mat %>%
    mutate(id = as.numeric(id))

  
  
df_place = as.data.frame(df_mat) %>% mutate(pol=1)
  names(df_place) = c("lat","long","id","pol")
  df_place = df_place %>% dplyr::filter(!is.na(lat)) %>%
    mutate(unique= paste0(id,pol))


sf1 <- sfheaders::sf_polygon(
    obj = df_place
    , x = "long"
    , y = "lat"
    , keep = T, polygon_id="id"
) 

sf1 = st_set_crs(sf1,st_crs(placeslist))


singles = left_join(sf1,indigTerr_mat)    

empties = singles %>% dplyr::filter(st_is_empty(singles$geometry))

  #I repeat the steps outlined above, this time for the 33 Indigenous territories that have multiple polygons associated with them, doing an additional loop through each polygon per territory, and giving a unique integer for each polygon.

empty_find = left_join(indigTerr_mat,sf1)

a = which(st_is_empty(empty_find$geometry))

  
indigTerr_mat2 = matrix(nrow=length(empties[[1]]),ncol=4)
df_mat2 = matrix(nrow=1000000,ncol=4)
idx=0



for (i in seq(1,33)) {
  if (!is.null(indigTerr[[a[[i]]]]$properties$Name)) {
    indigTerr_mat2[i,1] = indigTerr[[a[[i]]]]$properties$Name
  }
  if (!is.null(indigTerr[[a[[i]]]]$properties$ID)) {
    indigTerr_mat2[i,2] = indigTerr[[a[[i]]]]$properties$ID
  }
  if (!is.null(indigTerr[[a[[i]]]]$properties$Slug)) {
    indigTerr_mat2[i,3] = indigTerr[[a[[i]]]]$properties$Slug
  }

  for (j in seq(length(indigTerr[[a[[i]]]]$geometry$coordinates))) {
    for (k in seq(length(indigTerr[[a[[i]]]]$geometry$coordinates[[j]][[1]]))) {
    idx = idx+1
    df_mat2[idx,1] = indigTerr[[a[[i]]]]$geometry$coordinates[[j]][[1]][[k]][[2]]
    df_mat2[idx,2] = indigTerr[[a[[i]]]]$geometry$coordinates[[j]][[1]][[k]][[1]]
    df_mat2[idx,3] = indigTerr[[a[[i]]]]$properties$ID
    df_mat2[idx,4] = j

  }}
}

indigTerr_mat2 = as.data.frame(indigTerr_mat2)
names(indigTerr_mat2) = c("name","id","slug")
indigTerr_mat2 = indigTerr_mat2[1:3] %>%
  mutate(id = as.numeric(id))

df_place2 = as.data.frame(df_mat2)
  names(df_place2) = c("lat","long","id","pol")
  df_place2 = df_place2 %>% dplyr::filter(!is.na(lat)) %>%
    mutate(unique= paste0(id,pol))



sf2 <- sfheaders::sf_polygon(
    obj = df_place2
    , x = "long"
    , y = "lat" 
    , keep = T, polygon_id="unique"
) 



    
  
#I join all the polygons into one big sf object (indigTerr_sf) and I remove empty geometries (the index for the 33 territories with multipolygons were empty in the original dataframe)

sf2$id = substr(sf2$unique, start = 1, stop = 5)
sf2 = sf2 %>% mutate(id=as.numeric(id))
sf2 = st_set_crs(sf2,st_crs(placeslist))
mults = left_join(sf2,indigTerr_mat2)    
mults = mults %>% select(id,name,slug,geometry,pol)

singles$unique = NA
mults$unique =NA

sf1 = st_set_crs(sf1,st_crs(placeslist))

indigTerr_sf = rbind(singles,mults) %>% st_as_sf(crs="+proj=longlat +datum=WGS84")
#write.csv(indigTerr_sf,"indigTerr_sf.csv",row.names=FALSE)
indigTerr_ne = indigTerr_sf[!st_is_empty(indigTerr_sf),]

#but this map below of all the Indigenous territories is a little messed up. For one thing, territories like id = 36,191 and id = 39,156 are clearly being drawn "in the wrong direction." They're on the edges of the world map but drawn like they stretch across the entire world. More generally, the polygons here don't really look the same as they do on the native-land.ca website... Australia looks pretty good. But the polygons on top of Greenland and the Canadian Arctic look bad... which makes me think there's something wrong with the method I used above. Maybe the wrong datum/CRS? Not sure...

lats=c(-88,-85,83,87)
longs=c(-176,176,-170,172)

world= as.data.frame(lats) %>% cbind(as.data.frame(longs))
names(world) = c("lat","long")

world = st_as_sf(world,coords=c("long","lat"),crs="+proj=longlat +datum=WGS84")
tm_shape(osm.raster(world)) +tm_rgb() + 
tm_shape(indigTerr_ne[which(st_is_valid(indigTerr_ne)),]) + tm_polygons(alpha=0.5)


## So that motivated approach #2. This approach is much slower and more computationally expensive but I feel more confident in the results. Instead of getting all the Indigenous territories in a single API call, I use a different API, where native-land will give you all the territories associated with a particular point. I run this call 4- or 5000 times on all the sites in FAUNMAP! It takes hours, so I log the results in a csv and I have this method commented out here.

#indigMat = matrix(nrow=100000,ncol=6)

#idx_places=0
#for (j in seq(length(placeslist[[1]]))) {
#  long = placeslist$geometry[[j]][[1]]
#  lat= placeslist$geometry[[j]][[2]]
#  places = content(GET(paste0("https://native-land.ca/api/index.php?maps=languages&position=",lat,",",long)))
#  if (length(places) != 0) {
#  for (k in seq(length(places))) {
#    print(paste0(j, " ", k))
#  idx_places = idx_places + 1
#  indigMat[idx_places,1] = placeslist$siteid[[j]]
#  indigMat[idx_places,2] = placeslist$sitename[[j]]
#  indigMat[idx_places,3] = placeslist$sitedescription[[j]]
#  indigMat[idx_places,4] = places[[k]]$properties$Name
#  indigMat[idx_places,5] = places[[k]]$properties$ID
#  indigMat[idx_places,6] = places[[k]]$properties$Slug
#  }
#  }
  
#   if (length(places) == 0) {
#  idx_places = idx_places + 1
#  indigMat[idx_places,1] = placeslist$siteid[[j]]
#  indigMat[idx_places,2] = placeslist$sitename[[j]]
#  indigMat[idx_places,3] = placeslist$sitedescription[[j]]
#  indigMat[idx_places,4] = "na"
#  indigMat[idx_places,5] = "na"
#  indigMat[idx_places,6] = "na"
#  }
#}

#indigdf = as.data.frame(indigMat)
#names(indigdf) = c("siteid","sitename","sitedescription","indigName","indigID","indigSlug")
#indigdf = indigdf %>% dplyr::filter(!is.na(siteid))

#distinct_indig = indigdf %>% group_by(siteid,indigID) %>% distinct()  %>% dplyr::filter(indigName!="na")

#write.csv(distinct_indig,"distinct_indig.csv",row.names=FALSE)
    
distinct_indig = read.csv("distinct_indig.csv")
```

The 4451 FAUNMAP sites are on land traditionally held by 302 Indigenous nations. 273 of the FAUNMAP sites are not on land claimed by an Indigenous nation. (All this according to the native-land.ca maps.)

Site 24GF250 is the site claimed by the most nations: the eastern and western Dakota, Lakota, Crow, Cheyenne, Plains Cree, Gros Ventre, and Michif.

```{r native-land-stats, echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}


distinct_indig = distinct_indig %>% mutate(indigName = (stri_unescape_unicode(gsub("<U\\+(....)>", "\\\\u\\1", distinct_indig$indigName))))
distinct_list = distinct_indig %>% ungroup() %>% group_by(sitename) %>% count() %>% arrange(desc(n))


indig_sites = st_filter(placeslist,indigTerr_ne) 
indig_sites = indig_sites %>% unique()

sects = st_intersects(indig_sites,placeslist)
sect_mat = matrix(nrow=length(sects),ncol=100)
for (el in seq(length(sects))) {
  for (place in seq(length(sects[[el]]))) {
    sect_mat[el,place] = sects[[el]][[place]]
  }
}

sect_mat = as.data.frame(sect_mat)
sect_mat = 

datatable(distinct_list,rownames=FALSE,options = list(pageLength = 10), caption="Number of nations claiming the land of a Neotoma site")


indigNatsList = distinct_indig %>% group_by(indigName) %>% count() %>% arrange(desc(n))

datatable(indigNatsList,rownames=FALSE,options = list(pageLength = 10), caption="Number of Neotoma sites held by Indigenous nations")



```