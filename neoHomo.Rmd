---
title: "Culturally Sensitive Records in the Neotoma Paleoecology Database"
author: "Nick Hoffman"
date: "September 2024"
output:
  html_document:
    df_print: paged
    highlight: pygment
    keep_md: no
    toc: true
    number_sections: true
    toc_depth: 1
    toc_float: true
    theme: journal
editor_options:
    chunk_output_type: inline
---

<style type="text/css">
h2, h3, h4, h5, h6 {
  counter-reset: section;
}
p {
  font-size:18px;
}

ul {
  font-size:18px;
}

li {
  font-size:18px;
}
table {
   padding: 0;border-collapse: collapse;
   layout: fixed;
   width: 90%; }
table tr {
   border-top: 1px solid #cccccc;
   background-color: white;
   margin: 0;
   padding: 0; }
table tr:nth-child(2n) {
   background-color: #f8f8f8; }
table tr th {
   font-weight: bold;
   border: 1px solid #cccccc;
   margin: 0;
   padding: 6px 13px; }
table tr td {
   border: 1px solid #cccccc;
   margin: 0;
   padding: 6px 13px; }
table tr th :first-child, table tr td :first-child {
   margin-top: 0; }
table tr th :last-child, table tr td :last-child {
   margin-bottom: 0; }
.html-widget {
    margin: auto;
}
</style>

---

```{r, results='asis', echo = F}
toc_depth <- rmarkdown::metadata$output$html_document$toc_depth
sel <- paste0("h",(toc_depth+1):10, collapse = " > span, ")
cat(paste0("<style>",
           sel, 
           " > .header-section-number { display: none; } </style>"))
```

# Introduction

For stewards of data in which Indigenous peoples have rights (i.e., Indigenous data), ethical data stewardship requires honoring such rights. Theorists of Indigenous data sovereignty are exhorting stewards to honor these rights by aligning with the CARE principles of Indigenous data governance (Kukutai & Taylor, 2016; Carroll, 2020; Taitingfong forthcoming ?). The Neotoma Paleoecology Database is working to align with the CARE principles through its participation in the Ethical Open Science for Past Global Change Data Research Coordination Network.  

Neotoma holds paleodata of many proxy types (e.g., pollen, vertebrate fauna, diatoms) from across the world (Williams et al 2018). Expertise concerning the different proxies stewarded by Neotoma is distributed among different kinds of specialists (e.g., palynologists, vertebrate paleoecologists, diatomists), and knowledge of all the datasets in Neotoma is likewise distributed. Because they derive from Indigenous lands, or concern the human and nonhuman relations of Indigenous peoples, some of these datasets are Indigenous. But so far, there is no complete record of the extent of these Indigenous data in Neotoma.  

In order to achieve alignment with CARE, it is necessary to have a more complete accounting of 1) what Indigenous data Neotoma currently stewards and 2) how Neotoma’s stewardship of those data conforms or not to best practice. We present here an account of our attempt to perform such a data audit, as well as our preliminary findings. 

We searched programmatically through Neotoma’s metadata for any sensitive Indigenous data. Such sensitive Indigenous data include those that concern Indigenous ancestors (e.g., human skeletal elements and human-derived radiocarbon dates) or derive from sacred sites (e.g., burial mounds). We also searched for any records in which purposeful imprecision has been introduced into the site coordinates because introducing imprecision is one way to protect sensitive sites. Lastly, we searched for datasets that were collected from federally recognized Indigenous lands, since these may be most relevant to nation building activities of tribal governments. 

We documented sensitive Indigenous records, fuzzed sites, and data from federally recognized Indigenous lands. Although limitations in Neotoma’s metadata led us to systematically undercount or overcount records in different circumstances, we are hopeful that this preliminary inventory will spur us to better steward these data.


<i>Note: the following audit contains potentially sensitive information about Indigenous ancestors. Our intent is to expose this information in order to work toward better management in the future.</i>

# Approach

Our audit has two prongs. First, we intend to surface those data which most directly manifest a connection to colonial violence. In the case of Neotoma, these data include records of human skeletal elements from Indigenous lands, as well as radiocarbon dates which derive from such elements. These records stigmatize and objectify Indigenous ancestors. These data also include records from culturally sensitive areas, such as Indigenous burial mounds. Ethical investigation of such sensitive sites requires consultation with local tribal nations and adherence to cultural protocols, which have often been lacking in paleoecological studies to date. Conversely, as part of this first prong, we think it useful to surface those Neotoma records which exemplify CAREful research practice. These include records that introduce purposeful imprecision into site coordinates in order to protect culturally sensitive locations, as well as data deriving from studies which explicitly document a process of collaboration with local tribal nations. The second prong of our audit is meant to surface those Neotoma records which are useful to tribal nations for their self-governance (data for governance subprinciple of CARE). These include any Neotoma records which derive from federal Indigenous lands. 

We searched for the following kinds of records:
<ol>
<li>Indigenous ancestors' skeletal material,</li>
<li>radiocarbon dates derived from Indigenous ancestors,</li>
<li>records collected from culturally sensitive locations,</li>
<li>records from locations whose precise coordinates have been fuzzed, and</li>
<li>records from federally recognized Indigenous lands in the US, Canada, and Australia.</li></ol>


```{r setup, echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

library(neotoma2)
library(DT)
library(sf)
library(tidyverse)
library(httr)
library(jsonlite)
library(tmap)
library(osmdata)
library(rosm)
library(geojsonsf)
library(stringr)
library(leaflet)
sf_use_s2(FALSE)

setwd("../../")
rezes = read_sf("tl_2019_us_aiannh.shp") %>% dplyr::select(NAME)

canada_rezes = geojson_sf('https://proxyinternet.nrcan.gc.ca/arcgis/rest/services/CLSS-SATC/CLSS_Administrative_Boundaries/MapServer/0/query?outFields=*&where=1%3D1&f=geojson')
canada_rezes = canada_rezes %>% dplyr::rename('NAME' = 'adminAreaNameEng') %>% dplyr::select(NAME)
canada_rezes = st_transform(canada_rezes,crs="NAD83")
canada_rezes = canada_rezes %>% dplyr::mutate(NAME = str_to_title(NAME))

aust_ipa1 = geojson_sf("https://gis.environment.gov.au/gispubmap/rest/services/ogc_services/Indigenous_Protected_Areas/FeatureServer/0/query?where=NAME+BETWEEN+%27A%27+AND+%27LZ%27&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=&returnGeometry=true&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&gdbVersion=&historicMoment=&returnDistinctValues=false&returnIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&multipatchOption=xyFootprint&resultOffset=&resultRecordCount=&returnTrueCurves=false&returnExceededLimitFeatures=false&quantizationParameters=&returnCentroid=false&timeReferenceUnknownClient=false&sqlFormat=none&resultType=&featureEncoding=esriDefault&datumTransformation=&f=geojson")

aust_ipa2 = geojson_sf('https://gis.environment.gov.au/gispubmap/rest/services/ogc_services/Indigenous_Protected_Areas/FeatureServer/0/query?where=NAME+BETWEEN+%27LZ%27+AND+%27ZZ%27&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=&returnGeometry=true&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&gdbVersion=&historicMoment=&returnDistinctValues=false&returnIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&multipatchOption=xyFootprint&resultOffset=&resultRecordCount=&returnTrueCurves=false&returnExceededLimitFeatures=false&quantizationParameters=&returnCentroid=false&timeReferenceUnknownClient=false&sqlFormat=none&resultType=&featureEncoding=esriDefault&datumTransformation=&f=geojson')
aust_ipa = aust_ipa1 %>% rbind(aust_ipa2)
aust_ipa = aust_ipa1 %>% dplyr::select(NAME)
aust_ipa = st_transform(aust_ipa,crs="NAD83")

rezes = rezes %>% rbind(canada_rezes) %>% rbind(aust_ipa)

#aust_rezes = read_sf("Aboriginal_Communities_Town_Reserves_DPLH_002.shp")
all_sites = as.data.frame(matrix(ncol=2,nrow=0))
map_sites = as.data.frame(matrix(ncol=7,nrow=0))

fuzzed_sites = as.data.frame(matrix(ncol=7,nrow=0))
names(all_sites) = c("NAME","n")
names(map_sites) = c("siteid","NAME","lon","lat","datasettype","database","datasetid")
map_sites = st_as_sf(map_sites,coords=c("lon","lat"),crs="NAD83")
names(fuzzed_sites) = c("lon","lat","siteid","sitename","datasettype","database","datasetid")
for (i in c(2,3,4,5,6,7,10,11,12,13,14,15,17,18,19,20,22,23,25,26,27,28,29,30,31, 32,33,35,36,37,38,39,41,42)) {
  db = content(GET(paste0("https://api.neotomadb.org/v2.0/apps/constdb/datasets?dbid=",i)))$data
  if (length(db) >0) {
    #print(paste("DB ID:",i))
    db_mat = matrix(nrow=length(db),ncol=7)
    for (m in seq(length(db))) {
      if(!is.null(db[[m]]$coords[[1]])) {
        db_mat[[m,1]] = db[[m]]$coords[[1]]
        db_mat[[m,2]] = db[[m]]$coords[[2]]
        db_mat[[m,3]] = db[[m]]$siteid
        db_mat[[m,4]] = db[[m]]$sitename
        db_mat[[m,5]] = db[[m]]$datasettype[[1]]
        db_mat[[m,6]] = i
        db_mat[[m,7]] = db[[m]]$datasetid
      }
    }
    db_df = as.data.frame(db_mat)
    names(db_df) = c("lon","lat","siteid","sitename","datasettype","database","datasetid")
    db_df = db_df %>% drop_na() %>% mutate(lon=as.numeric(lon),lat=as.numeric(lat))
    fuzzed = db_df %>% dplyr::filter((lon %% 0.25 == 0) & (lat %% 0.25 ==0))
    if (length(fuzzed[[1]]) != 0 ) {
      fuzzed_sites = rbind(fuzzed_sites,fuzzed)
    }
    fuzzed2 = db_df %>% drop_na(lon) %>% drop_na(lat) %>% dplyr::mutate(lon_decimals = nchar(str_extract(lon, "\\.(\\d+)")) - 1) %>%
      dplyr::mutate(lat_decimals = nchar(str_extract(lat, "\\.(\\d+)")) - 1) %>%
      dplyr::mutate(lat_decimals = case_when(is.na(lat_decimals) ~ 0, TRUE ~ lat_decimals)) %>%
      dplyr::mutate(lon_decimals = case_when(is.na(lon_decimals) ~ 0, TRUE ~ lon_decimals)) %>%
      dplyr::filter(lon_decimals <= 1 | lat_decimals <= 1) %>%
      dplyr::select(lon,lat,siteid,sitename,datasettype,database,datasetid)
        if (length(fuzzed[[2]]) != 0 ) {
      fuzzed_sites = rbind(fuzzed_sites,fuzzed2) %>% distinct()
    }
    db_df = distinct(db_df) %>% st_as_sf(coords=c("lon","lat"),crs="NAD83")
   
    sites_by_rez = st_join(db_df,rezes) %>% group_by(NAME) %>% count() %>% arrange(desc(n)) %>% drop_na()
    sites_by_rez_map = st_join(db_df,rezes) %>% drop_na(NAME)
  #  datatable(st_drop_geometry(sites_by_rez), rownames = FALSE)
    if(length(sites_by_rez[[1]])!=0) {
        all_sites = rbind(all_sites,sites_by_rez)
        map_sites = rbind(map_sites, sites_by_rez_map)
        assign(paste0('sites_by_rez_',i),st_drop_geometry(sites_by_rez))
    }
    }
}




db_names = c("African Pollen Database", "European Pollen Database", "Indo-Pacific Pollen Database", 
"Latin American Pollen Database", "North American Pollen Database", 
"Pollen Database of Siberia and the Russian Far East", "FAUNMAP", "Neotoma", 
"North American Plant Macrofossil Database", "Academy of Natural Sciences of Drexel University", 
"NDSU Insect Database", "North American Non-Marine Ostracode Database Project (NANODe)", 
"Alaskan Archaeofaunas","French Institute of Pondicherry Palynology and Paleoecology Database" ,"Japanese Pollen Database", "Neotoma Midden Database", 
"Chinese Pollen Database","Holocene Perspective on Peatland Biogeochemistry" ,"Neotoma Testate Amoebae Database", 
"Deep-Time Palynology Database", "Neotoma Biomarker Database", 
"Alpine Palynological Database", "Canadian Museum of Nature-Delorme Ostracoda-Surface Samples", 
"Diatom Paleolimnology Data Cooperative (DPDC)", "Neotoma Ostracode Database", 
"Faunal Isotope Database", "Neotoma Charcoal Data","Pollen Monitoring Programme", "PaleoVertebrates of Latin America", 
"St. Croix Watershed Research Station of the Science Museum of Minnesota", 
"Tropical South American Diatom Database", "Marine Dinoflagellates Database", "Nonmarine Ostracod Distribution in Europe Database","East Asian Nonmarine Ostracod Database") %>% as.data.frame()

db_names = db_names %>% cbind(as.data.frame(c(2,3,4,5,6,7,10,11,12,13,14,15,17,18,19,20,22,23,25,26,27,28,29,30,31, 32,33,35,36,37,38,39,41,42)))


names(db_names) = c("db_names","database")
db_names = db_names %>% dplyr::mutate(database = as.character(database))


all_sites = all_sites %>% group_by(NAME) %>% summarize(n=sum(n)) %>% arrange(desc(n)) 


map_sites = map_sites %>% distinct()

map_sites_table = map_sites %>% left_join(db_names) %>% dplyr::select(NAME,sitename,datasettype,db_names)%>% rename("database" = "db_names")%>% dplyr::filter(datasettype != "geochronologic") %>% group_by(database,datasettype) %>% count() %>% arrange(desc(n))

fuzzed_sites = fuzzed_sites %>% distinct() %>% st_as_sf(coords=c("lon","lat"),crs="NAD83")


map_sites_neo = get_sites(c(as.numeric(map_sites$siteid)),all_data=TRUE)

reps = ceiling(length(rezes[[1]])/11)
colors = rep(c("red","blue","limegreen","purple","yellow","black","orange","skyblue","pink","brown","darkgreen"),reps)

colors = colors[1:length(rezes[[1]])]
library(rmapshaper)
rez2 = rezes %>% dplyr::filter(!NAME %in% c('Murray Lake 4','Nunavut Land Claims Agreement \u2013 Hall Beach Inuit Owned Land','Nunavut Land Claims Agreement \u2013 Bathurst Inlet Inuit Owned Land','Nunavut Land Claims Agreement \u2013 Repulse Bay Inuit Owned Land','Nunavut Land Claims Agreement \u2013 Taloyoak Inuit Owned Lands','Isadore Harry 12','Ochapowace I.r. 71-131','Nunavut Land Claims Agreement \u2013 Arctic Bay Inuit Owned Land'))
test = ms_simplify(rez2)


fuzzed_sites = fuzzed_sites %>% dplyr::filter(datasettype != "geochronologic")

num_fuzzed= length(fuzzed_sites[[1]])


#repospec prep
repospecs = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=repositoryspecimens&count=false&limit=99999&offset=0"))$data

repospec_mat = matrix(nrow=length(repospecs),ncol=5)

for (i in seq(length(repospecs))) {
  for(j in seq(5) ) {
    if (!is.null(repospecs[[i]][[j]])) {
  repospec_mat[i,j] = repospecs[[i]][[j]]
    }}}


repospec_df = as.data.frame(repospec_mat)


names(repospec_df) = c("datasetid","repositoryid","notes","recdatecreated","recdatemodified")

#specimens prep
specs = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=specimens&count=false&limit=99999&offset=0"))$data

spec_mat = matrix(nrow=length(specs),ncol=17)

for (i in seq(length(specs))) {
  for(j in seq(17) ) {
    if (!is.null(specs[[i]][[j]])) {
  spec_mat[i,j] = specs[[i]][[j]]
    }}
  }


spec_df = as.data.frame(spec_mat)


names(spec_df) = c("specimenid","dataid","elementtypeid","symmetryid","portionid","maturityid","sexid","domesticstatusid","preservative","nisp","repositoryid","specimennr","fieldnr","arctosnr","notes","recdatecreated","recdatemodified")


filtered_repospec = repospec_df %>% dplyr::filter(datasetid %in% map_sites$datasetid)



database_list = c("Cooperative Holocene Mapping Project", "African Pollen Database", "European Pollen Database", "Indo-Pacific Pollen Database", "Latin American Pollen Database", "North American Pollen Database", "Pollen Database of Siberia and the Russian Far East", "Canadian Pollen Database", "FAUNMAP", "Neotoma", "North American Plant Macrofossil Database", "Academy of Natural Sciences of Drexel University", "NDSU Insect Database", "North American Non-Marine Ostracode Database Project (NANODe)", "MioMap", "Alaskan Archaeofaunas", "French Institute of Pondicherry Palynology and Paleoecology Database", "Japanese Pollen Database", "Neotoma Midden Database", "Chinese Pollen Database", "Holocene Perspective on Peatland Biogeochemistry", "ANTIGUA", "Neotoma Testate Amoebae Database", "Deep-Time Palynology Database", "Neotoma Biomarker Database", "Alpine Pollen Database", "Canadian Museum of Nature-Delorme Ostracoda-Surface Samples", "Diatom Paleolimnology Data Cooperative (DPDC)", "Neotoma Ostracode Database", "Faunal Isotope Database", "Neotoma Charcoal Data", "Pollen Monitoring Programme", "PaleoVertebrates of Latin America", "St. Croix Watershed Research Station of the Science Museum of Minnesota","Marine Dinoflagellates Database","Tropical South American Diatom Database", "Packrat Middens", "Sedimentary aDNA Database")







```


# Are any samples in Neotoma from humans?

## Method
We downloaded Neotoma's taxa table and selected any taxon IDs which might describe people. See table below. (Taxon ID 6359 is Primates, and 6171 is Mammalia.)

```{r get-taxa,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
## get taxa
taxalist = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=taxa&limit=75000&offset=0"))$data

taxa_df = matrix(nrow=length(taxalist),ncol=14)

for (i in seq(1,length(taxalist))) {
  for (j in seq(1,14)) {
    if (!is.null(taxalist[[i]][[j]])) {
      taxa_df[i,j] = taxalist[[i]][[j]]
    }
  }
}

taxa_df = as.data.frame(taxa_df)

names(taxa_df) = names(taxalist[[1]])

test1 = grep("Homo ",taxa_df$taxonname)

homos = taxa_df[c(grep("Homo ",taxa_df$taxonname),grep("^Homo$", taxa_df$taxonname),grep("^Homini", taxa_df$taxonname)),]

datatable(homos[c(1,3,6,7)],rownames = FALSE)

## relevant indices are 5571, 6261, 6631

##higher taxa indices are 6261, 6262, 5623


## get occurrences from taxon id
homolist = content(GET("https://api.neotomadb.org/v2.0/data/taxa/6116,6821,6822,7196/occurrences?limit=2500&offset=0"))$data

```


Then we used a Neotoma API to search for any occurrences of those taxon IDs.

The map below shows the sites where human samples come from, and the table documents what information there is about those samples. Rows colored red are sensitivity level 1 because they come from North America. Rows colored orange are sensitivity level 2 because they come from elsewhere.

It should be noted that lead FAUNMAP steward Jessica Blois has removed all sample-level Homo sapiens occurrences from public access as FAUNMAP works on a policy for managing these data.

```{r occurrences, echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}

homo_df = matrix(nrow=length(homolist),ncol=15)

for (i in seq(1,length(homolist))) {
  for (j in seq(1,1)) {
    if (!is.null(homolist[[i]][[j]])) {
      homo_df[i,j] = homolist[[i]][[j]]
    }
    
  }
   for (k in seq(1,4)) {
    if (!is.null(homolist[[i]][[2]][[k]])) {
      homo_df[i,(1+k)] = homolist[[i]][[2]][[k]]
    }}
   for (k in seq(1,3)) {
    if (!is.null(homolist[[i]][[3]][[k]])) {
      homo_df[i,(5+k)] = homolist[[i]][[3]][[k]]
    } }
  
   for (k in seq(1,7)) {
    if (!is.null(homolist[[i]][[4]][[k]])) {
      homo_df[i,(8+k)] = homolist[[i]][[4]][[k]]
    }}
}

homo_df = as.data.frame(homo_df)

names(homo_df) = c("occid","taxonid","taxonname","value","sampleunits","age","ageolder","ageyounger","datasetid","siteid","sitename","altitude","location","datasettype","database")

homo_sites = homo_df %>% dplyr::select(siteid) %>% distinct()

homo_sites_neo = get_sites(c(as.numeric(homo_sites[[1]])))

homo_sites_sf = as.data.frame(homo_sites_neo) %>%
  st_as_sf(coords=c("long","lat"),crs="+proj=longlat +datum=WGS84")

homo_full = left_join(homo_sites_sf,homo_df)

homo_bg = osm.raster(homo_sites_sf)


#tm_shape(homo_bg)+
#    tm_rgb() +
#    tm_shape(homo_full) +
#    tm_dots("database",size=0.1,alpha=0.4,palette=c('black','red','green',"purple")) +
#    tm_layout(legend.position=c("RIGHT","BOTTOM"),
#              legend.bg.color="white",
#              legend.bg.alpha=0.9,
#              legend.width=0.4,
#              legend.text.size=0.5,
#              main.title= 'Sites with samples deriving from #humans', 
#              main.title.position = "center",
#              title.bg.color = "white", panel.label.height=1)
```

``` {r another, echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}
#<h3 style="text-align:center;">Human Ancestor Sites</h3>
plotLeaflet(homo_sites_neo) %>%
   addPolygons(data = test,
        stroke = FALSE, fillOpacity = 0.5, smoothFactor = 0.5,
        fillColor = ~colors,
        popup= ~NAME)


homo_table = homo_df[c(1,3,4,5,7,8,9,10,11,15)]



sitestring = paste0(homo_df$siteid,collapse=",")

gps = content(GET(paste0("https://api.neotomadb.org/v2.0/data/sites/",sitestring,"/geopoliticalunits?limit=250&offset=0")))$data

gps_mat = matrix(nrow=52,ncol=6)
idx_sites = 0
for (i in seq(length(gps))) {
  for (j in seq(length(gps[[i]]$geopoliticalunits))) {
    idx_sites = idx_sites + 1
    if (!is.null(gps[[i]]$siteid)) {
    gps_mat[idx_sites,1] = gps[[i]]$siteid}
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[1]])) {
    gps_mat[idx_sites,2] = gps[[i]]$geopoliticalunits[[j]][[1]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[2]])) {
    gps_mat[idx_sites,3] = gps[[i]]$geopoliticalunits[[j]][[2]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[3]])) {
    gps_mat[idx_sites,4] = gps[[i]]$geopoliticalunits[[j]][[3]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[4]])) {
    gps_mat[idx_sites,5] = gps[[i]]$geopoliticalunits[[j]][[4]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[5]])) {
    gps_mat[idx_sites,6] = gps[[i]]$geopoliticalunits[[j]][[5]] }
  }
}


gps_df = as.data.frame(gps_mat)

names(gps_df) = c("siteid","rank","gp_id","gp_name","gp_unit","higher_gp_id")

#datatable(gps_df,rownames=FALSE)

gps_sum1 = gps_df %>% dplyr::filter(rank==1) 

homo_table = left_join(homo_table,gps_sum1,by="siteid") %>% dplyr::rename("country" = "gp_name") %>% dplyr::mutate(country = as.factor(country)) %>% mutate(sensitivity = case_when(country == "United States" | country == "Mexico" | country == "Canada" ~ 1, TRUE ~ 2))

datatable(homo_table[c(2:6,8,9,13,16)],rownames=FALSE) %>% formatStyle(target="row",
  'sensitivity',
  backgroundColor = styleEqual(c(1), c('rgba(255,0,0,0.5)'),default="rgba(255, 165, 0, 0.5)")
)


#,fontWeight = 'bold'
```


```{r polygons, echo=FALSE,include=TRUE,message = FALSE}


#Below I count the occurrences by database and taxon.

#counttabl = homo_df %>% group_by(database,taxonname) %>% count() %>% arrange(desc(n))

#gps_sum2 = gps_df %>% dplyr::filter(rank==1) %>% group_by(gp_name) %>% count() %>% arrange(desc(n))

#datatable(counttabl,rownames = FALSE)

```


```{r countries, echo=FALSE,include=TRUE,message = FALSE}
#Below I list all geopolitical units associated with any of the 22 distinct sites.

```


The table below counts sample records by sensitivity and constituent database. 
```{r highest, echo=FALSE,include=TRUE,message = FALSE}


#Lastly, I count the countries the distinct sites are in.

#datatable(gps_sum2,rownames=FALSE)


homo_table2 = homo_table %>% group_by(database,sensitivity) %>% count() %>% arrange(desc(n))

datatable(homo_table2)

```


## Next steps

Our next steps are to reach out to the lead stewards for the Faunal Isotope Database, PaVeLa and FAUNMAP, so they can come to a decision about managing these human records in their databases.

# Are any of Neotoma's radiocarbon dates derived from humans?

## Methods

We searched through two fields (<i>notes</i> and <i>materialdated</i>) from Neotoma's geochronology table for any occurrences of words from the dictionary below. 

```{r geochron,echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

#geochrons prep
geochrons = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=geochronology&count=false&limit=99999&offset=0"))$data

geochron_mat = matrix(nrow=length(geochrons),ncol=14)

for (i in seq(length(geochrons))) {
  for(j in seq(14) ) {
    if (!is.null(geochrons[[i]][[j]])) {
  geochron_mat[i,j] = geochrons[[i]][[j]]
    }}}


geochron_df = as.data.frame(geochron_mat)


names(geochron_df) = c("geochronid","sampleid","geochrontypeid","agetypeid","age","errorolder","erroryounger","infinite","delta13c","labnumber","materialdated","notes","recdatecreated","recdatemodified")

#collunits prep
collunits = content(GET("https://api.neotomadb.org/v2.0/data/dbtables/collectionunits?count=false&limit=99999"))$data


collunits_mat = matrix(nrow=length(collunits),ncol=20)

for (i in seq(length(collunits))) {
  for(j in seq(20) ) {
    if (!is.null(collunits[[i]][[j]])) {
  collunits_mat[i,j] = collunits[[i]][[j]]
    }}}


collunits_df = as.data.frame(collunits_mat)


names(collunits_df) = c("collectionunitid","handle","siteid","colltypeid","depenvtid","collunitname","colldate","colldevice","gpslatitude","gpslongitude","gpsaltitude","gpserror","waterdepth","substrateid","slopeaspect","slopeangle","location","notes","recdatecreated","recdatemodified")


#
materialtypes = geochron_df %>% dplyr::group_by(materialdated) %>% count()


dictionary = c("human","Human","Homo","homo","indian","Indian","native","Native","indigenous","Indigenous","mound","Mound","buri","Buri","bury","Bury","archaeo","Archaeo","person","Person","people","People","cairn","Cairn")

## geochrons materialDated

matches_geochron_materialDated <- grep(paste(dictionary,collapse="|"), 
                        geochron_df[[11]])

geochron_results= geochron_df[matches_geochron_materialDated,]


## geochrons notes
matches_geochron_notes <- grep(paste(dictionary,collapse="|"), 
                        geochron_df[[12]])


geochron_results= rbind(geochron_results, geochron_df[matches_geochron_notes,]) %>% distinct()


## collunits  location

matches_collunits_location <- grep(paste(dictionary,collapse="|"), 
                        collunits_df$location)


collunits_results= collunits_df[matches_collunits_location,]


## collunits notes

matches_collunits_notes <- grep(paste(dictionary,collapse="|"), 
                        collunits_df$notes)

collunits_results = rbind(collunits_results,collunits_df[matches_collunits_notes,]) %>% distinct()


dictionary = as.data.frame(dictionary)
dictionary <- dictionary[order(dictionary$dictionary),]

datatable(as.data.frame(dictionary),rownames=FALSE)


```


Any rows from the geochronology table which contained one of the above words is listed in the table below. Notice that not all of these radiocarbon dates are <i>necessarily</i> problematic, only potentially. Further scrutiny may be needed. (We also checked against CARD's list of radiocarbon dates deriving from human ancestors that are duplicated in Neotoma, and there was agreement between the two lists: all 60 of CARD's records that are also in Neotoma are in the below table.)

We assigned sensitivity categories as follows: any references to human bone were assigned sensitivity level 1. Any references to human feces were assigned sensitivity level 2. References to human graves or burials also merited a 2. All other items were given sensitivity level 3. All publications linked to records in which the material dated was taxon-ambiguous bone collagen were consulted. We found that geochron IDs 21255, 29333, 29334, and 29335 definitely derive from humans. These records were therefore categorized as sensitivity level 1.

Below the color-coded table, we count records by their sensitivity and the constituent database of which they are a part.

```{r geochron-results,echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

samples = list()
for (i in seq(20)) {
  val=i*30000 - 30000
  samples = append(samples,content(GET(paste0('https://api.neotomadb.org/v2.0/data/dbtables/samples?count=false&limit=30000&offset=',val)))$data)
}



sample_mat = matrix(nrow=length(samples),ncol=2)

for (i in seq(length(samples))) {
    if (!is.null(samples[[i]]$sampleid)) {
  sample_mat[i,1] = samples[[i]]$sampleid
    }
      if (!is.null(samples[[i]]$datasetid)) {
  sample_mat[i,2] = samples[[i]]$datasetid
    }}


sample_df = as.data.frame(sample_mat)


names(sample_df) = c("sampleid","datasetid")


sample_filtered = sample_df %>% dplyr::filter(sampleid %in% geochron_results$sampleid) 

sample_topaste = sample_filtered %>% dplyr::select(datasetid) %>% distinct()

ds_info1 = content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/",paste0(sample_topaste[1:25,],collapse=","))))$data


ds_info2 = content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/",paste0(sample_topaste[26:length(sample_topaste[,1]),],collapse=","))))$data

ds_info = append(ds_info1,ds_info2)


ds_mat = matrix(nrow=length(ds_info),ncol=3)


for (i in seq(length(ds_info))) {
    if (!is.null(ds_info[[i]]$site$datasets[[1]]$datasetid)) {
  ds_mat[i,1] = ds_info[[i]]$site$datasets[[1]]$datasetid
    }
      if (!is.null(ds_info[[i]]$site$datasets[[1]]$datasettype)) {
  ds_mat[i,2] = ds_info[[i]]$site$datasets[[1]]$datasettype
      }
      if (!is.null(ds_info[[i]]$site$datasets[[1]]$database)) {
  ds_mat[i,3] = ds_info[[i]]$site$datasets[[1]]$database
    }
}
ds_df = as.data.frame(ds_mat)


names(ds_df) = c("datasetid","datasettype","database")
ds_df = ds_df %>% dplyr::mutate(datasetid = as.numeric(datasetid))
join_df = sample_filtered %>% dplyr::left_join(ds_df)



chronpubs = content(GET('https://api.neotomadb.org/v2.0/data/dbtables/geochronpublications?count=false&limit=99999&offset=0'))$data


chronpub_mat = matrix(nrow=length(chronpubs),ncol=4)

for (i in seq(length(chronpubs))) {
  for(j in seq(4) ) {
    if (!is.null(chronpubs[[i]][[j]])) {
  chronpub_mat[i,j] = chronpubs[[i]][[j]]
    }}}


chronpub_df = as.data.frame(chronpub_mat)


names(chronpub_df) = c("geochronid","publicationid","recdatecreated","recdatemodified")


geochron_results = geochron_results %>% dplyr::mutate(sensitivity =  case_when(
                                                          (grepl("Homo",materialdated,fixed= TRUE) | grepl("homo",materialdated,fixed= TRUE) | grepl("human",materialdated,fixed= TRUE) | grepl("Human",materialdated,fixed= TRUE)) & (grepl("bone",materialdated,fixed= TRUE) | grepl("Bone",materialdated,fixed= TRUE) | grepl("skull",materialdated,fixed= TRUE) | grepl("Skull",materialdated,fixed= TRUE))
| grepl(21255,geochronid,fixed= TRUE)| grepl(29333,geochronid,fixed= TRUE)| grepl(29334,geochronid,fixed= TRUE)| grepl(29335,geochronid,fixed= TRUE)
 ~ 1,          (grepl("Homo",materialdated,fixed= TRUE) | grepl("homo",materialdated,fixed= TRUE) | grepl("human",materialdated,fixed= TRUE) | grepl("Human",materialdated,fixed= TRUE)) & (grepl("dung",materialdated,fixed= TRUE) | grepl("Dung",materialdated,fixed= TRUE) | grepl("coprolite",materialdated,fixed= TRUE) | grepl("Coprolite",materialdated,fixed= TRUE) | grepl("feces",materialdated,fixed= TRUE) | grepl("Feces",materialdated,fixed= TRUE)) ~ 2, grepl("human burial", materialdated, fixed=TRUE) ~ 2, grepl("human grave",notes,fixed=TRUE) ~ 2,TRUE ~ 3))


geochron_investigate = geochron_results %>% dplyr::filter(sensitivity==3 & (grepl("bone",materialdated,fixed=TRUE) | grepl("Bone",materialdated,fixed=TRUE)))

pub_IDs = chronpub_df %>% dplyr::filter(geochronid %in% geochron_investigate$geochronid)

pub_investigate = content(GET(paste0("https://api.neotomadb.org/v2.0/data/publications/",paste0(pub_IDs$publicationid,collapse=","))))$data


pub_mat = matrix(nrow=length(pub_investigate),ncol=2)

for (i in seq(length(pub_investigate))) {
    if (!is.null(pub_investigate[[i]]$publication$publicationid)) {
  pub_mat[i,1] = pub_investigate[[i]]$publication$publicationid
    }
     if (!is.null(pub_investigate[[i]]$publication$citation)) {
  pub_mat[i,2] = pub_investigate[[i]]$publication$citation
    }}


pub_df = as.data.frame(pub_mat)


names(pub_df) = c("publicationid","citation")

newview = left_join(geochron_investigate,chronpub_df,by = join_by(geochronid)) %>% left_join(pub_df) %>% dplyr::select(geochronid,publicationid,citation)

radiocarbon = content(GET('https://api.neotomadb.org/v2.0/data/dbtables/radiocarbon?count=false&limit=99999&offset=0'))$data


rc_mat = matrix(nrow=length(radiocarbon),ncol=10)

for (i in seq(length(radiocarbon))) {
  if (!is.null(radiocarbon[[i]]$geochronid[[1]])) {
    rc_mat[i,1] = radiocarbon[[i]]$geochronid[[1]]
  }
    if (!is.null(radiocarbon[[i]]$radiocarbonmethodid[[1]])) {
    rc_mat[i,2] = radiocarbon[[i]]$radiocarbonmethodid[[1]]
  }
  for(j in seq(6) ) {
    if (!is.null(radiocarbon[[i]][[(j+2)]])) {
  rc_mat[i,(j+2)] = radiocarbon[[i]][[(j+2)]]
    }}
   for(j in seq(2) ) {
    if (!is.null(radiocarbon[[i]][[(j+8)]])) {
  rc_mat[i,(j+8)] = radiocarbon[[i]][[(j+8)]]
    }}}


rc_df = as.data.frame(rc_mat)


names(rc_df) = c("geochronid","radiocarbonmethodid","percentc","percentn","delta13c","delta15n","percentcollagen","reservoir","masscmg","cnratio")


check = rc_df %>% dplyr::filter(geochronid %in% c(29333,29334,29335))

datatable(geochron_results[c(1,2,5,6,7,10,11,12,15)],rownames=FALSE) %>% formatStyle(target="row",
  'sensitivity',
  backgroundColor = styleEqual(c(1,2,3), c('rgba(255, 0, 0, 0.5)','rgba(255, 165, 0, 0.5)','rgba(0, 255, 0, 0.5)'),default="white")
)

join_df = join_df %>% dplyr::mutate(sampleid = as.character(sampleid))
geochron_counter = geochron_results %>% left_join(join_df) %>% group_by(database,sensitivity) %>% count() %>% drop_na() %>% arrange(desc(n))

datatable(geochron_counter,rownames=FALSE)

```

## Next steps
Reach out to stewards of relevant constituent databases and ask them to come to a decision about managing these records.

# Are any of the collection units for Neotoma's records from culturally sensitive areas?

## Methods
We used the same dictionary from the last query to search through two fields in Neotoma's collection units table (<i>location</i> and <i>notes</i>). Any collection units that returned one of the above words is reproduced below. The records were individually scrutinized categorized subjectively into sensitivity categories.

```{r collunits,echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}





collunits_results = collunits_results %>% dplyr::mutate(sensitivity =                                     case_when(grepl("human burial", notes, fixed=TRUE) ~ 2, grepl("human grave",notes,fixed=TRUE) ~ 2, grepl("Burial vessel",location,fixed=TRUE)|grepl("Burial objects",location,fixed=TRUE)|grepl("Burial artefacts",location,fixed=TRUE)|grepl("Mound",location,fixed=TRUE)|grepl("Indian camp",notes,fixed=TRUE)|grepl("human habitation",notes,fixed=TRUE)|grepl("by humans",notes,fixed=TRUE) ~ 2,grepl("Human burial",notes,fixed=TRUE)|grepl("cairn",notes,fixed=TRUE)|grepl("Human remains",notes,fixed=TRUE)|grepl("Human and caribou blood",notes,fixed=TRUE) ~ 1,
                                         TRUE ~ 3))



datatable(collunits_results[c(1,3,6,7,17,18,20,21)],rownames=FALSE) %>% formatStyle(target="row",
  'sensitivity',
  backgroundColor = styleEqual(c(1,2,3), c('rgba(255, 0, 0, 0.5)','rgba(255, 165, 0, 0.5)','rgba(0, 255, 0, 0.5)'),default="white")
)



depenvtypes = content(GET("https://api.neotomadb.org/v2.0/data/dbtables/depenvttypes?count=false&limit=1000&offset=0"))$data

depenv_mat = matrix(nrow=length(depenvtypes),ncol=length(depenvtypes[[1]]))
for (i in seq(length(depenvtypes))) {
  for (j in seq(length(depenvtypes[[1]]))) {
    if(!is.null(depenvtypes[[i]][[j]])) {
      depenv_mat[[i,j]] = depenvtypes[[i]][[j]]
    }
  }
}

depenv_df = as.data.frame(depenv_mat)
names(depenv_df) = c("depenvtid","depenvt","depenvthigherid","recdatecreated","recdatemodified")

#archaeological_envs = depenv_df %>% dplyr::filter(depenvthigherid==1)


#archaeo_collunits = collunits_df %>% dplyr::filter(depenvtid %in% archaeological_envs$depenvtid)


#number_archaeo_collunits = length(archaeo_collunits[,1])
```
We counted the number of records by their constituent database and by their sensitivity. Notice that the count here is greater than the total number of collectionunits because constituent databases are linked to datasets, not collection units, and multiple datasets can derive from a single collection unit. (We did exclude the Neotoma datasettype "geochronologic".)

```{r depenvtyps, echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}



sites = content(GET(paste0("https://api.neotomadb.org/v1.5/data/sites/",paste0(collunits_results$siteid,collapse=","))))$data





site_mat = matrix(nrow=length(sites),ncol=length(sites[[1]]))
for (i in seq(length(sites))) {
  for (j in seq(length(sites[[1]]))) {
    if(!is.null(sites[[i]][[j]])) {
      site_mat[[i,j]] = sites[[i]][[j]]
    }
  }
}

site_df = as.data.frame(site_mat)
names(site_df) = c("siteid","sitename","sitedescription","geography","altitude","collectionunitid","collectionunit","handle","unittype","datasetid","datasettype")

site_df = site_df %>% dplyr::filter(collectionunitid %in% collunits_results$collectionunitid) 
datasets = site_df %>% dplyr::filter(datasettype != 'geochronologic') %>% dplyr::select(datasetid) %>% distinct()

ds_list = list()
for (i in seq(14)) {
  val = 25*i
  val0 = val - 24
  ds_list = append(ds_list,content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/",paste0(datasets[val0:val,],collapse=","))))$data)
}

summer=0
for (i in seq(length(ds_list))) {
  summer = summer + length(ds_list[[i]]$site$datasets)
}

ds_mat2 = matrix(nrow=summer,ncol=3)


idx= 0
for (i in seq(length(ds_list))) {
  for (j in seq(length(ds_list[[i]]$site$datasets))) {
    idx = idx + 1
    if (!is.null(ds_list[[i]]$site$datasets[[j]]$datasetid)) {
  ds_mat2[idx,1] = ds_list[[i]]$site$datasets[[j]]$datasetid
    }
      if (!is.null(ds_list[[i]]$site$datasets[[j]]$datasettype)) {
  ds_mat2[idx,2] = ds_list[[i]]$site$datasets[[j]]$datasettype
      }
      if (!is.null(ds_list[[i]]$site$datasets[[j]]$database)) {
  ds_mat2[idx,3] = ds_list[[i]]$site$datasets[[j]]$database
      }
    }
}
ds_df2 = as.data.frame(ds_mat2)


names(ds_df2) = c("datasetid","datasettype","database")

joiner = left_join(site_df,ds_df2) %>% dplyr::select(collectionunitid,datasetid,datasettype,database)

coll_joined = collunits_results %>% dplyr::left_join(joiner) %>% dplyr::filter(datasettype != "geochronologic") %>% group_by(database,sensitivity) %>% count() %>% arrange(desc(n)) %>% drop_na(database)


datatable(coll_joined,rownames=FALSE)
#We also took another approach, wherein we selected any depostional environment IDs that corresponded to archaeological sites (see table below), and filtered for all collection units that corresponded to any of those depositional environment IDs. This returned `r number_archaeo_collunits` results.

#datatable(archaeological_envs,rownames=FALSE)

#datatable(archaeo_collunits[c(1,2,3,6,7,8,17,18,20)],rownames=FALSE)

```
## Next steps
Our next steps are to reach out to the relevant constituent database stewards for sensitivity levels 1 and 2 and ask them to come to a decision about managing these records.


# Are the coordinates for any sites in Neotoma fuzzed?

## Methods
We count any site whose geography is provided as a bounding box rather a point as fuzzed because according to <a target="_blank" href="https://neotoma-manual.readthedocs.io/en/latest/tables_site.html#sites">the Neotoma Manual</a>, "the lat-long box can be used either to circumscribe the areal extent of a site or to provide purposeful imprecision to the site location."  Notice that this is a liberal definition - some sites with bounding box geographies will have been so formatted for reasons other than purposeful imprecision.  

We found `r num_fuzzed` fuzzed sites using this method. One table below documents the site names, dataset types, and constituent databases associated with fuzzed sites. The next table counts datasets associated with fuzzed sites by the type of dataset and the constituent database from which the dataset derives, and the map below documents fuzzed site locations.

```{r fuzzed,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}

#We checked for all sites whether both the latitude and longitude were exactly divisible by 0.25, or whether latitude or the longitude had a precision of 0 or 1 decimal places. We said those sites which met this definition were fuzzed. (Notice that this is a conservative method. There are likely fuzzed sites in Neotoma whose coordinates are not exactly divisible by 0.25, or that have precision greater than 1 decimal place).

#fuzzed_sites_neo = get_sites(c(as.numeric(fuzzed_sites$siteid)),all_data=TRUE)

#fuzzed_by_dbset = fuzzed_sites %>% group_by(database,datasettype) %>% count() %>% arrange(desc(n))

#fuzzed_by_dbset = fuzzed_by_dbset %>% left_join(db_names) %>% dplyr::select(db_names,datasettype,n) %>% rename("database" = "db_names")
#fuzzed_sites = fuzzed_sites %>% left_join(db_names) %>% dplyr::select(sitename,datasettype,db_names)%>% rename("database" = "db_names")

#datatable(st_drop_geometry(fuzzed_sites),rownames=FALSE)


#datatable(st_drop_geometry(fuzzed_by_dbset),rownames=FALSE) 



#tm_shape(osm.raster(fuzzed_sites)) + tm_rgb() +
#  tm_shape(rezes) + tm_fill(col="blue",alpha=0.8) +
#  tm_shape(fuzzed_sites) + tm_dots(col="red",alpha=0.3,size=0.02,border.alpha=0.3) +
#  tm_add_legend(type = "fill", 
#                  col = c("blue"),
#                  labels = c("Federal Indigenous land"),
#                  title = "Legend",
#                  border.col="blue") +
#  tm_add_legend(type = "symbol", 
#                  col = c("red"),
#                  labels = c("Fuzzed Neotoma Site"),
#                  title = "",
#                  border.col="red")



idx=0
for (i in seq(1,length(database_list))) {
  #print(i)
  db_format = gsub(" ", '%20', database_list[i], fixed=TRUE)
  returns = content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/db?limit=10000&offset=0&database=",db_format)))$data
  if(length(returns) != 0) {
    for (j in seq(length(returns))) {
      if(!is.null(returns[[j]]$site$geography)) {
      if (st_geometry_type(geojson_sf(returns[[j]]$site$geography)) != 'POINT') {
        for (k in seq(length(returns[[j]]$site$datasets))) {
          idx = idx + 1
        }
        }
        }
    }
  }
  
}



site_mat = matrix(nrow=idx,ncol=5)
idx=0
for (i in seq(1,length(database_list))) {
  db_format = gsub(" ", '%20', database_list[i], fixed=TRUE)
  returns = content(GET(paste0("https://api.neotomadb.org/v2.0/data/datasets/db?limit=10000&offset=0&database=",db_format)))$data
  if(length(returns) != 0) {
    for (j in seq(length(returns))) {
      if(!is.null(returns[[j]]$site$geography)) {
      if (st_geometry_type(geojson_sf(returns[[j]]$site$geography)) != 'POINT') {
        for (k in seq(length(returns[[j]]$site$datasets))) {
          idx = idx + 1
          if(!is.null(returns[[j]]$site$sitename )) {
            site_mat[idx,1] = returns[[j]]$site$sitename }
          if(!is.null(returns[[j]]$site$geography )) {
            site_mat[idx,2] = returns[[j]]$site$geography }
          if(!is.null(returns[[j]]$site$siteid )) {
            site_mat[idx,3] = returns[[j]]$site$siteid }
          if(!is.null(returns[[j]]$site$datasets[[k]]$datasettype )) {
            site_mat[idx,4] = returns[[j]]$site$datasets[[k]]$datasettype}
          if(!is.null(returns[[j]]$site$datasets[[k]]$database )) {
            site_mat[idx,5] = returns[[j]]$site$datasets[[k]]$database}
        }
        }
        }
    }
  }
  
}

site_df = as.data.frame(site_mat)
names(site_df)= c("site_name","geography","siteid","datasettype","database")


fuzzed_datasets = site_df %>% dplyr::filter(datasettype != "geochronologic")


fuzzed_set_counter = fuzzed_datasets %>% group_by(database,datasettype) %>% count() %>% arrange(desc(n))
 

distinct_sites = site_df %>% dplyr::select(site_name,siteid,geography) %>% distinct()

num_fuzzed = length(distinct_sites[[1]])

datatable(fuzzed_datasets[c(1,4,5)],rownames=FALSE)



datatable(fuzzed_set_counter,rownames=FALSE)

fuzzed_sites_neo = get_sites(c(as.numeric(distinct_sites$siteid)),all_data=TRUE)

plotLeaflet(fuzzed_sites_neo) %>%
   addPolygons(data = test,
        stroke = FALSE, fillOpacity = 0.5, smoothFactor = 0.5,
        fillColor = ~colors,
        popup= ~NAME)


```

## Next steps
Our next steps are to refine our definition of fuzzed sites.


# Are any sites in Neotoma located on federal Indigenous lands?

## Method
We did a spatial join for every site in Neotoma with a unique site ID to shapefiles of the borders of federal Indigenous lands in <a target="_blank" href="https://catalog.data.gov/dataset/tiger-line-shapefile-2019-nation-u-s-current-american-indian-alaska-native-native-hawaiian-area">the United States</a> and <a target="_blank" href="https://hub.arcgis.com/datasets/esrica-tsg::indigenous-lands-of-canada/about">Canada</a>, and Indigenous protected areas in <a href="https://fed.dcceew.gov.au/datasets/75c48afce3bb445f9ce58633467e21ed/about" target="_blank">Australia</a>, and we tallied and mapped all those which intersected the borders of federal reservations. See list below.


```{r cars,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
## https://hub.arcgis.com/datasets/esrica-tsg::indigenous-lands-of-canada/about


datatable(st_drop_geometry(all_sites),rownames=FALSE)

plotLeaflet(map_sites_neo) %>% 
  addTiles() %>% 
  addPolygons(data = test,
        stroke = FALSE, fillOpacity = 0.5, smoothFactor = 0.5,
        fillColor = ~colors,
        popup= ~NAME)

```

Next, we counted those Neotoma datasets derived sites which are on federal Indigenous lands by the Neotoma constituent database with which they are associated and the kind of dataset they are.

``` {r add, echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

datatable(st_drop_geometry(map_sites_table),rownames=FALSE)


#tm_shape(osm.raster(map_sites)) + tm_rgb() +
#  tm_shape(rezes) + tm_fill(col="blue",alpha=0.8) +
#  tm_shape(map_sites) + tm_dots(col="red",alpha=0.3,size=0.02,border.alpha=0.3) +
#  tm_add_legend(type = "fill", 
#                  col = c("blue"),
##                  labels = c("Federal Indigenous land"),
#                  title = "Legend",
#                  border.col="blue") +
#  tm_add_legend(type = "symbol", 
#                  col = c("red"),
#                  labels = c("Neotoma Site"),
#                  title = "",
#                  border.col="red")

```


``` {r any-specs,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}


```

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>