---
title: "Culturally Sensitive Records in the Neotoma Paleoecology Database"
author: "Nick Hoffman"
date: "August 2024"
output:
  html_document:
    df_print: paged
    highlight: pygment
    keep_md: no
    toc: true
    number_sections: true
    toc_depth: 1
    toc_float: true
    theme: journal
editor_options:
    chunk_output_type: inline
---

<style type="text/css">
h2, h3, h4, h5, h6 {
  counter-reset: section;
}
p {
  font-size:18px;
}

ul {
  font-size:18px;
}

li {
  font-size:18px;
}
table {
   padding: 0;border-collapse: collapse;
   layout: fixed;
   width: 90%; }
table tr {
   border-top: 1px solid #cccccc;
   background-color: white;
   margin: 0;
   padding: 0; }
table tr:nth-child(2n) {
   background-color: #f8f8f8; }
table tr th {
   font-weight: bold;
   border: 1px solid #cccccc;
   margin: 0;
   padding: 6px 13px; }
table tr td {
   border: 1px solid #cccccc;
   margin: 0;
   padding: 6px 13px; }
table tr th :first-child, table tr td :first-child {
   margin-top: 0; }
table tr th :last-child, table tr td :last-child {
   margin-bottom: 0; }
.html-widget {
    margin: auto;
}
</style>

---

```{r, results='asis', echo = F}
toc_depth <- rmarkdown::metadata$output$html_document$toc_depth
sel <- paste0("h",(toc_depth+1):10, collapse = " > span, ")
cat(paste0("<style>",
           sel, 
           " > .header-section-number { display: none; } </style>"))
```

# Introduction

The intention of this data audit is to find any records from the Neotoma Paleoecology Database which potentially violate Neotoma's statement of values, especially with respect to Neotoma's goal of aligning with principles of Indigenous data sovereignty. The authors of this audit are white settler accomplices informed by guidance from theorists of Indigenous data sovereignty, paleo scientists, and other colleagues.

Note: the following audit contains potentially sensitive information about Indigenous ancestors. Our intent is to expose this information in order to work toward better management in the future.

# Are any sites in Neotoma located on federal Indigenous lands?

## Method
We did a spatial join for every site in Neotoma with a unique site ID to shapefiles of the borders of federal Indigenous in <a target="_blank" href="https://catalog.data.gov/dataset/tiger-line-shapefile-2019-nation-u-s-current-american-indian-alaska-native-native-hawaiian-area">the United States</a> and <a target="_blank" href="https://hub.arcgis.com/datasets/esrica-tsg::indigenous-lands-of-canada/about">Canada</a>, and Indigenous protected areas in <a href="https://fed.dcceew.gov.au/datasets/75c48afce3bb445f9ce58633467e21ed/about" target="_blank">Australia</a>, and we tallied and mapped all those which intersected the borders of federal reservations. See list below.


```{r setup, echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

library(neotoma2)
library(DT)
library(sf)
library(tidyverse)
library(httr)
library(jsonlite)
library(tmap)
library(osmdata)
library(rosm)
library(geojsonsf)
library(stringr)
library(leaflet)
sf_use_s2(FALSE)
```


```{r cars,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
## https://hub.arcgis.com/datasets/esrica-tsg::indigenous-lands-of-canada/about
setwd("C:/Users/Nick/Documents")
rezes = read_sf("tl_2019_us_aiannh.shp") %>% dplyr::select(NAME)

canada_rezes = geojson_sf('https://proxyinternet.nrcan.gc.ca/arcgis/rest/services/CLSS-SATC/CLSS_Administrative_Boundaries/MapServer/0/query?outFields=*&where=1%3D1&f=geojson')
canada_rezes = canada_rezes %>% dplyr::rename('NAME' = 'adminAreaNameEng') %>% dplyr::select(NAME)
canada_rezes = st_transform(canada_rezes,crs="NAD83")

aust_ipa = geojson_sf("https://gis.environment.gov.au/gispubmap/rest/services/ogc_services/Indigenous_Protected_Areas/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson")
aust_ipa = aust_ipa %>% dplyr::select(NAME)
aust_ipa = st_transform(aust_ipa,crs="NAD83")

rezes = rezes %>% rbind(canada_rezes) %>% rbind(aust_ipa)

#aust_rezes = read_sf("Aboriginal_Communities_Town_Reserves_DPLH_002.shp")
all_sites = as.data.frame(matrix(ncol=2,nrow=0))
map_sites = as.data.frame(matrix(ncol=6,nrow=0))

fuzzed_sites = as.data.frame(matrix(ncol=6,nrow=0))
names(all_sites) = c("NAME","n")
names(map_sites) = c("siteid","NAME","lon","lat","datasettype","database")
map_sites = st_as_sf(map_sites,coords=c("lon","lat"),crs="NAD83")
names(fuzzed_sites) = c("lon","lat","siteid","sitename","datasettype","database")
for (i in c(2,3,4,5,6,7,10,11,12,13,14,15,17,18,19,20,22,23,25,26,27,28,29,30,31, 32,33,35,36,37,38,39,41,42)) {
  db = content(GET(paste0("https://api.neotomadb.org/v2.0/apps/constdb/datasets?dbid=",i)))$data
  if (length(db) >0) {
    #print(paste("DB ID:",i))
    db_mat = matrix(nrow=length(db),ncol=6)
    for (m in seq(length(db))) {
      if(!is.null(db[[m]]$coords[[1]])) {
        db_mat[[m,1]] = db[[m]]$coords[[1]]
        db_mat[[m,2]] = db[[m]]$coords[[2]]
        db_mat[[m,3]] = db[[m]]$siteid
        db_mat[[m,4]] = db[[m]]$sitename
        db_mat[[m,5]] = db[[m]]$datasettype[[1]]
        db_mat[[m,6]] = i
      }
    }
    db_df = as.data.frame(db_mat)
    names(db_df) = c("lon","lat","siteid","sitename","datasettype","database")
    db_df = db_df %>% drop_na() %>% mutate(lon=as.numeric(lon),lat=as.numeric(lat))
    fuzzed = db_df %>% dplyr::filter((lon %% 0.25 == 0) & (lat %% 0.25 ==0))
    if (length(fuzzed[[1]]) != 0 ) {
      fuzzed_sites = rbind(fuzzed_sites,fuzzed)
    }
    fuzzed2 = db_df %>% drop_na(lon) %>% drop_na(lat) %>% dplyr::mutate(lon_decimals = nchar(str_extract(lon, "\\.(\\d+)")) - 1) %>%
      dplyr::mutate(lat_decimals = nchar(str_extract(lat, "\\.(\\d+)")) - 1) %>%
      dplyr::mutate(lat_decimals = case_when(is.na(lat_decimals) ~ 0, TRUE ~ lat_decimals)) %>%
      dplyr::mutate(lon_decimals = case_when(is.na(lon_decimals) ~ 0, TRUE ~ lon_decimals)) %>%
      dplyr::filter(lon_decimals <= 1 | lat_decimals <= 1) %>%
      dplyr::select(lon,lat,siteid,sitename,datasettype,database)
        if (length(fuzzed[[2]]) != 0 ) {
      fuzzed_sites = rbind(fuzzed_sites,fuzzed2) %>% distinct()
    }
    db_df = distinct(db_df) %>% st_as_sf(coords=c("lon","lat"),crs="NAD83")
   
    sites_by_rez = st_join(db_df,rezes) %>% group_by(NAME) %>% count() %>% arrange(desc(n)) %>% drop_na()
    sites_by_rez_map = st_join(db_df,rezes) %>% drop_na(NAME)
  #  datatable(st_drop_geometry(sites_by_rez), rownames = FALSE)
    if(length(sites_by_rez[[1]])!=0) {
        all_sites = rbind(all_sites,sites_by_rez)
        map_sites = rbind(map_sites, sites_by_rez_map)
        assign(paste0('sites_by_rez_',i),st_drop_geometry(sites_by_rez))
    }
    }
}

all_sites = all_sites %>% group_by(NAME) %>% summarize(n=sum(n)) %>% arrange(desc(n)) %>% st_drop_geometry()
map_sites = map_sites %>% distinct()

fuzzed_sites = fuzzed_sites %>% distinct() %>% st_as_sf(coords=c("lon","lat"),crs="NAD83")
datatable(all_sites,rownames=FALSE)

tm_shape(osm.raster(map_sites)) + tm_rgb() +
  tm_shape(rezes) + tm_fill(col="blue",alpha=0.8) +
  tm_shape(map_sites) + tm_dots(col="red",alpha=0.3,size=0.02,border.alpha=0.3) +
  tm_add_legend(type = "fill", 
                  col = c("blue"),
                  labels = c("Federal Indigenous land"),
                  title = "Legend",
                  border.col="blue") +
  tm_add_legend(type = "symbol", 
                  col = c("red"),
                  labels = c("Neotoma Site"),
                  title = "",
                  border.col="red")

map_sites_neo = get_sites(c(as.numeric(map_sites$siteid)),all_data=TRUE)

reps = ceiling(length(rezes[[1]])/11)
colors = rep(c("red","blue","limegreen","purple","yellow","black","orange","skyblue","pink","brown","darkgreen"),reps)

colors = colors[1:length(rezes[[1]])]
library(rmapshaper)
test = ms_simplify(rezes)

plotLeaflet(map_sites_neo) %>% 
  addTiles() %>% 
  addPolygons(data = test,
        stroke = FALSE, fillOpacity = 0.5, smoothFactor = 0.5,
        fillColor = ~colors,
        popup= ~NAME)

fuzzed_sites = fuzzed_sites %>% dplyr::filter(datasettype != "geochronologic")

num_fuzzed= length(fuzzed_sites[[1]])

```

## Next Steps
Our next steps are...

# Are the coordinates for any sites in Neotoma fuzzed?

## Methods
We checked for all sites whether both the latitude and longitude were exactly divisible by 0.25, or any sites where latitude or the longitude has a precision of 0 or 1 decimal places. If they were, we said they were fuzzed. Notice that this is a conservative method. There are likely fuzzed sites in Neotoma whose coordinates are not exactly divisible by 0.25, or that have precision greater than 1 decimal place. We found `r num_fuzzed` such fuzzed sites. One table below documents their siteids and names, the next table counts datasets by the type of dataset and the constituent database from which the dataset derives, and the map below documents their locations.

```{r fuzzed,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}

fuzzed_sites_neo = get_sites(c(as.numeric(fuzzed_sites$siteid)),all_data=TRUE)

fuzzed_by_dbset = fuzzed_sites %>% group_by(database,datasettype) %>% count() %>% arrange(desc(n))


db_names = c("African Pollen Database", "European Pollen Database", "Indo-Pacific Pollen Database", 
"Latin American Pollen Database", "North American Pollen Database", 
"Pollen Database of Siberia and the Russian Far East", "FAUNMAP", "Neotoma", 
"North American Plant Macrofossil Database", "Academy of Natural Sciences of Drexel University", 
"NDSU Insect Database", "North American Non-Marine Ostracode Database Project (NANODe)", 
"Alaskan Archaeofaunas","French Institute of Pondicherry Palynology and Paleoecology Database" ,"Japanese Pollen Database", "Neotoma Midden Database", 
"Chinese Pollen Database","Holocene Perspective on Peatland Biogeochemistry" ,"Neotoma Testate Amoebae Database", 
"Deep-Time Palynology Database", "Neotoma Biomarker Database", 
"Alpine Palynological Database", "Canadian Museum of Nature-Delorme Ostracoda-Surface Samples", 
"Diatom Paleolimnology Data Cooperative (DPDC)", "Neotoma Ostracode Database", 
"Faunal Isotope Database", "Neotoma Charcoal Data","Pollen Monitoring Programme", "PaleoVertebrates of Latin America", 
"St. Croix Watershed Research Station of the Science Museum of Minnesota", 
"Tropical South American Diatom Database", "Marine Dinoflagellates Database", "Nonmarine Ostracod Distribution in Europe Database","East Asian Nonmarine Ostracod Database") %>% as.data.frame()


db_names = db_names %>% cbind(as.data.frame(c(2,3,4,5,6,7,10,11,12,13,14,15,17,18,19,20,22,23,25,26,27,28,29,30,31, 32,33,35,36,37,38,39,41,42)))


names(db_names) = c("db_names","database")
db_names = db_names %>% dplyr::mutate(database = as.character(database))

fuzzed_by_dbset = fuzzed_by_dbset %>% left_join(db_names) %>% dplyr::select(db_names,datasettype,n)
datatable(st_drop_geometry(fuzzed_by_dbset[]),rownames=FALSE) 

datatable(st_drop_geometry(fuzzed_sites),rownames=FALSE)
tm_shape(osm.raster(fuzzed_sites)) + tm_rgb() +
  tm_shape(rezes) + tm_fill(col="blue",alpha=0.8) +
  tm_shape(fuzzed_sites) + tm_dots(col="red",alpha=0.3,size=0.02,border.alpha=0.3) +
  tm_add_legend(type = "fill", 
                  col = c("blue"),
                  labels = c("Federal Indigenous land"),
                  title = "Legend",
                  border.col="blue") +
  tm_add_legend(type = "symbol", 
                  col = c("red"),
                  labels = c("Fuzzed Neotoma Site"),
                  title = "",
                  border.col="red")

plotLeaflet(fuzzed_sites_neo) %>%
   addPolygons(data = test,
        stroke = FALSE, fillOpacity = 0.5, smoothFactor = 0.5,
        fillColor = ~colors,
        popup= ~NAME)


```

## Next steps
Our next steps are to refine our definition of fuzzed sites and to speak with constituent database stewards.

# Are any samples in Neotoma from humans?

## Method
We downloaded Neotoma's taxa table and selected any taxon IDs which might describe people.(Taxon ID 6359 is Primates, and 6171 is Mammalia.)

```{r get-taxa,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
## get taxa
taxalist = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=taxa&limit=75000&offset=0"))$data

taxa_df = matrix(nrow=length(taxalist),ncol=14)

for (i in seq(1,length(taxalist))) {
  for (j in seq(1,14)) {
    if (!is.null(taxalist[[i]][[j]])) {
      taxa_df[i,j] = taxalist[[i]][[j]]
    }
  }
}

taxa_df = as.data.frame(taxa_df)

names(taxa_df) = names(taxalist[[1]])

test1 = grep("Homo ",taxa_df$taxonname)

homos = taxa_df[c(grep("Homo ",taxa_df$taxonname),grep("^Homo$", taxa_df$taxonname),grep("^Homini", taxa_df$taxonname)),]

datatable(homos[c(1,3,6,7)],rownames = FALSE)

## relevant indices are 5571, 6261, 6631

##higher taxa indices are 6261, 6262, 5623


## get occurrences from taxon id
homolist = content(GET("https://api.neotomadb.org/v2.0/data/taxa/6116,6821,6822,7196/occurrences?limit=2500&offset=0"))$data

```


Then we used a Neotoma API to search for any occurrences of those taxon IDs.

The two maps below show the sites they come from, and the table documents what information there is about those samples from the samples table in Neotoma. Rows colored red are sensitivity level 1 because they come from the United States. Rows colored orange are sensitivity level 2 because they come from elsewhere.

It should be noted that lead FAUNMAP steward Jessica Blois has removed all sample-level Homo sapiens occurrences from public access as the Database works on a policy for managing these data.

```{r occurrences, echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}

homo_df = matrix(nrow=length(homolist),ncol=15)

for (i in seq(1,length(homolist))) {
  for (j in seq(1,1)) {
    if (!is.null(homolist[[i]][[j]])) {
      homo_df[i,j] = homolist[[i]][[j]]
    }
    
  }
   for (k in seq(1,4)) {
    if (!is.null(homolist[[i]][[2]][[k]])) {
      homo_df[i,(1+k)] = homolist[[i]][[2]][[k]]
    }}
   for (k in seq(1,3)) {
    if (!is.null(homolist[[i]][[3]][[k]])) {
      homo_df[i,(5+k)] = homolist[[i]][[3]][[k]]
    } }
  
   for (k in seq(1,7)) {
    if (!is.null(homolist[[i]][[4]][[k]])) {
      homo_df[i,(8+k)] = homolist[[i]][[4]][[k]]
    }}
}

homo_df = as.data.frame(homo_df)

names(homo_df) = c("occid","taxonid","taxonname","value","sampleunits","age","ageolder","ageyounger","datasetid","siteid","sitename","altitude","location","datasettype","database")

homo_sites = homo_df %>% dplyr::select(siteid) %>% distinct()

homo_sites_neo = get_sites(c(as.numeric(homo_sites[[1]])))

homo_sites_sf = as.data.frame(homo_sites_neo) %>%
  st_as_sf(coords=c("long","lat"),crs="+proj=longlat +datum=WGS84")

homo_full = left_join(homo_sites_sf,homo_df)

homo_bg = osm.raster(homo_sites_sf)


tm_shape(homo_bg)+
    tm_rgb() +
    tm_shape(homo_full) +
    tm_dots("database",size=0.1,alpha=0.4,palette=c('black','red','green',"purple")) +
    tm_layout(legend.position=c("RIGHT","BOTTOM"),
              legend.bg.color="white",
              legend.bg.alpha=0.9,
              legend.width=0.4,
              legend.text.size=0.5,
              main.title= 'Sites with samples deriving from humans', 
              main.title.position = "center",
              title.bg.color = "white", panel.label.height=1)
```

``` {r another, echo=FALSE,include=TRUE,message = FALSE}
#<h3 style="text-align:center;">Human Ancestor Sites</h3>
plotLeaflet(homo_sites_neo) %>%
   addPolygons(data = test,
        stroke = FALSE, fillOpacity = 0.5, smoothFactor = 0.5,
        fillColor = ~colors,
        popup= ~NAME)


homo_table = homo_df[c(1,3,4,5,7,8,9,10,11,15)]



sitestring = paste0(homo_df$siteid,collapse=",")

gps = content(GET(paste0("https://api.neotomadb.org/v2.0/data/sites/",sitestring,"/geopoliticalunits?limit=250&offset=0")))$data

gps_mat = matrix(nrow=52,ncol=6)
idx_sites = 0
for (i in seq(length(gps))) {
  for (j in seq(length(gps[[i]]$geopoliticalunits))) {
    idx_sites = idx_sites + 1
    if (!is.null(gps[[i]]$siteid)) {
    gps_mat[idx_sites,1] = gps[[i]]$siteid}
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[1]])) {
    gps_mat[idx_sites,2] = gps[[i]]$geopoliticalunits[[j]][[1]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[2]])) {
    gps_mat[idx_sites,3] = gps[[i]]$geopoliticalunits[[j]][[2]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[3]])) {
    gps_mat[idx_sites,4] = gps[[i]]$geopoliticalunits[[j]][[3]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[4]])) {
    gps_mat[idx_sites,5] = gps[[i]]$geopoliticalunits[[j]][[4]] }
    if (!is.null(gps[[i]]$geopoliticalunits[[j]][[5]])) {
    gps_mat[idx_sites,6] = gps[[i]]$geopoliticalunits[[j]][[5]] }
  }
}


gps_df = as.data.frame(gps_mat)

names(gps_df) = c("siteid","rank","gp_id","gp_name","gp_unit","higher_gp_id")

#datatable(gps_df,rownames=FALSE)

gps_sum1 = gps_df %>% dplyr::filter(rank==1) 

homo_table = left_join(homo_table,gps_sum1,by="siteid") %>% dplyr::rename("country" = "gp_name") %>% dplyr::mutate(country = as.factor(country)) %>% mutate(sensitivity = case_when(country == "United States" | country == "Mexico" | country == "Canada" ~ 1, TRUE ~ 2))

datatable(homo_table[c(2:6,8,9,13,16)],rownames=FALSE) %>% formatStyle(target="row",
  'sensitivity',
  backgroundColor = styleEqual(c(1), c('rgba(255,0,0,0.5)'),default="rgba(255, 165, 0, 0.5)")
)


#,fontWeight = 'bold'
```


```{r polygons, echo=FALSE,include=TRUE,message = FALSE}


#Below I count the occurrences by database and taxon.

#counttabl = homo_df %>% group_by(database,taxonname) %>% count() %>% arrange(desc(n))

#gps_sum2 = gps_df %>% dplyr::filter(rank==1) %>% group_by(gp_name) %>% count() %>% arrange(desc(n))

#datatable(counttabl,rownames = FALSE)

```


```{r countries, echo=FALSE,include=TRUE,message = FALSE}
#Below I list all geopolitical units associated with any of the 22 distinct sites.

```


The table below counts sample records by sensitivity and constituent database. 
```{r highest, echo=FALSE,include=TRUE,message = FALSE}


#Lastly, I count the countries the distinct sites are in.

#datatable(gps_sum2,rownames=FALSE)


homo_table2 = homo_table %>% group_by(database,sensitivity) %>% count() %>% arrange(desc(n))

datatable(homo_table2)

```


## Next steps

Our next steps are to reach out to the lead stewards for the Faunal Isotope Dtabase and PaVeLa, so they can come to a decision about managing these human records in their databases.

# Are any of Neotoma's radiocarbon dates derived from humans?

## Methods

We searched through two fields (<i>notes</i> and <i>materialdated</i>) from Neotoma's geochronology table for any occurrences of words from the dictionary below. 

```{r geochron,echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

#geochrons prep
geochrons = content(GET("https://api.neotomadb.org/v2.0/data/dbtables?table=geochronology&count=false&limit=99999&offset=0"))$data

geochron_mat = matrix(nrow=length(geochrons),ncol=14)

for (i in seq(length(geochrons))) {
  for(j in seq(14) ) {
    if (!is.null(geochrons[[i]][[j]])) {
  geochron_mat[i,j] = geochrons[[i]][[j]]
    }}}


geochron_df = as.data.frame(geochron_mat)


names(geochron_df) = c("geochronid","sampleid","geochrontypeid","agetypeid","age","errorolder","erroryounger","infinite","delta13c","labnumber","materialdated","notes","recdatecreated","recdatemodified")

#collunits prep
collunits = content(GET("https://api.neotomadb.org/v2.0/data/dbtables/collectionunits?count=false&limit=99999"))$data


collunits_mat = matrix(nrow=length(collunits),ncol=20)

for (i in seq(length(collunits))) {
  for(j in seq(20) ) {
    if (!is.null(collunits[[i]][[j]])) {
  collunits_mat[i,j] = collunits[[i]][[j]]
    }}}


collunits_df = as.data.frame(collunits_mat)


names(collunits_df) = c("collectionunitid","handle","siteid","colltypeid","depenvtid","collunitname","colldate","colldevice","gpslatitude","gpslongitude","gpsaltitude","gpserror","waterdepth","substrateid","slopeaspect","slopeangle","location","notes","recdatecreated","recdatemodified")


#
materialtypes = geochron_df %>% dplyr::group_by(materialdated) %>% count()


dictionary = c("human","Human","Homo","homo","indian","Indian","native","Native","indigenous","Indigenous","mound","Mound","buri","Buri","bury","Bury","archaeo","Archaeo","person","Person","people","People","cairn","Cairn")

## geochrons materialDated

matches_geochron_materialDated <- grep(paste(dictionary,collapse="|"), 
                        geochron_df[[11]])

geochron_results= geochron_df[matches_geochron_materialDated,]


## geochrons notes
matches_geochron_notes <- grep(paste(dictionary,collapse="|"), 
                        geochron_df[[12]])


geochron_results= rbind(geochron_results, geochron_df[matches_geochron_notes,]) %>% distinct()


## collunits  location

matches_collunits_location <- grep(paste(dictionary,collapse="|"), 
                        collunits_df$location)


collunits_results= collunits_df[matches_collunits_location,]


## collunits notes

matches_collunits_notes <- grep(paste(dictionary,collapse="|"), 
                        collunits_df$notes)

collunits_results = rbind(collunits_results,collunits_df[matches_collunits_notes,]) %>% distinct()


dictionary = as.data.frame(dictionary)
dictionary <- dictionary[order(dictionary$dictionary),]

datatable(as.data.frame(dictionary),rownames=FALSE)


```


Any rows from the geochronology table which contained one of the above words is listed in the table below. Notice that not all of these radiocarbon dates is <i>necessarily</i> problematic, only potentially. Further scrutiny may be needed. (We also checked against CARD's list of radiocarbon dates deriving from human ancestors that are duplicated in Neotoma, and there was agreement between the two lists: all 60 of CARD's records that are also in Neotoma are in the below table.)

We assigned sensitivity categories as follows: any references to human bone were assigned sensitivity level 1. Any references to human feces were assigned sensitivity level 2. All other items were given sensitivity level 3. It should be noted that references to bone collagen without taxon are ambiguous and may require reference back to original papers to determine taxonomic source. In particular, the bone collagen from a burial cairn may require further scrutiny.

```{r geochron-results,echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}


#samples1 = content(GET('https://api.neotomadb.org/v2.0/data/dbtables/samples?count=false&limit=99999&offset=0'))$data

geochron_results = geochron_results %>% dplyr::mutate(sensitivity = 
                                                        case_when(
                                                          (grepl("Homo",materialdated,fixed= TRUE) | grepl("homo",materialdated,fixed= TRUE) | grepl("human",materialdated,fixed= TRUE) | grepl("Human",materialdated,fixed= TRUE)) & (grepl("bone",materialdated,fixed= TRUE) | grepl("Bone",materialdated,fixed= TRUE) | grepl("skull",materialdated,fixed= TRUE) | grepl("Skull",materialdated,fixed= TRUE)) ~ 1,
                                         (grepl("Homo",materialdated,fixed= TRUE) | grepl("homo",materialdated,fixed= TRUE) | grepl("human",materialdated,fixed= TRUE) | grepl("Human",materialdated,fixed= TRUE)) & (grepl("dung",materialdated,fixed= TRUE) | grepl("Dung",materialdated,fixed= TRUE) | grepl("coprolite",materialdated,fixed= TRUE) | grepl("Coprolite",materialdated,fixed= TRUE) | grepl("feces",materialdated,fixed= TRUE) | grepl("Feces",materialdated,fixed= TRUE)) ~ 2,
                                         TRUE ~ 3))

datatable(geochron_results[c(1,2,5,6,7,10,11,12,15)],rownames=FALSE) %>% formatStyle(target="row",
  'sensitivity',
  backgroundColor = styleEqual(c(1,2,3), c('rgba(255, 0, 0, 0.5)','rgba(255, 165, 0, 0.5)','rgba(0, 255, 0, 0.5)'),default="white")
)




```

## Next steps
Actually scrutinizes these records - publications associated with taxon-ambiguous bone collagen samples. Link these records to constituent databases. Then reach out to stewards of relevant constituent databases.


# Are any of the collection units for Neotoma's records from culturally sensitive areas?

## Methods
We used the same dictionary from the last query to search through two fields in Neotoma's collection units table (<i>location</i> and <i>notes</i>). Any collection units that returned one of the above words is reproduced below.

```{r collunits,echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}


datatable(collunits_results[c(1,2,3,6,7,8,17,18,20)],rownames=FALSE)



depenvtypes = content(GET("https://api.neotomadb.org/v2.0/data/dbtables/depenvttypes?count=false&limit=1000&offset=0"))$data

depenv_mat = matrix(nrow=length(depenvtypes),ncol=length(depenvtypes[[1]]))
for (i in seq(length(depenvtypes))) {
  for (j in seq(length(depenvtypes[[1]]))) {
    if(!is.null(depenvtypes[[i]][[j]])) {
      depenv_mat[[i,j]] = depenvtypes[[i]][[j]]
    }
  }
}

depenv_df = as.data.frame(depenv_mat)
names(depenv_df) = c("depenvtid","depenvt","depenvthigherid","recdatecreated","recdatemodified")

archaeological_envs = depenv_df %>% dplyr::filter(depenvthigherid==1)


archaeo_collunits = collunits_df %>% dplyr::filter(depenvtid %in% archaeological_envs$depenvtid)


number_archaeo_collunits = length(archaeo_collunits[,1])
```

We also took another approach, wherein we selected any depostional environment IDs that corresponded to archaeological sites (see table below), and filtered for all collection units that corresponded to any of those depositional environment IDs. This returned `r number_archaeo_collunits` results.

```{r depenvtyps, echo=FALSE,include=TRUE,message = FALSE, warning=FALSE}

datatable(archaeological_envs,rownames=FALSE)

datatable(archaeo_collunits[c(1,2,3,6,7,8,17,18,20)],rownames=FALSE)

```
## Next steps
Need to actually scrutinize these records.